{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AutoTokenizer, AutoModel\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../../data/twitter-distaster/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 5), (3263, 4))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f\"{data_dir}/train.csv\")\n",
    "test_df = pd.read_csv(f\"{data_dir}/test.csv\")\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = train_df[51:70]\n",
    "train_df = train_df[0:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 466,
       "width": 708
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.target.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "\n",
    "  def __init__(\n",
    "    self, \n",
    "    data: pd.DataFrame, \n",
    "    tokenizer: BertTokenizer, \n",
    "    max_token_len: int = 512,\n",
    "    text_col: str = \"text\",\n",
    "    label_col: str = \"target\"\n",
    "  ):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.data = data\n",
    "    self.max_token_len = max_token_len\n",
    "    self.text_col = text_col\n",
    "    self.label_col = label_col\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index: int):\n",
    "    data_row = self.data.iloc[index]\n",
    "\n",
    "    comment_text = data_row[self.text_col]\n",
    "    labels = data_row[self.label_col]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      comment_text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_token_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "      comment_text=comment_text,\n",
    "      input_ids=encoding[\"input_ids\"].flatten(),\n",
    "      attention_mask=encoding[\"attention_mask\"].flatten(),\n",
    "      labels=torch.tensor(labels)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['comment_text', 'input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = TwitterDataset(\n",
    "  train_df,\n",
    "  tokenizer,\n",
    "  max_token_len=512\n",
    ")\n",
    "\n",
    "sample_item = train_dataset[0]\n",
    "sample_item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_item[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London is cool ;)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a wonderful day!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No way...I can't eat that shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love my girlfriend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you like pasta?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The end!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>54</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>@PhDSquares #mufc they've built so much hype a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>55</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>World Wide!!</td>\n",
       "      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>56</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>57</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Paranaque City</td>\n",
       "      <td>Ablaze for you Lord :D</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>59</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>61</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the outside you're ablaze and alive\\nbut yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>62</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>milky way</td>\n",
       "      <td>Had an awesome time visiting the CFC head offi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>63</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I wanted to set Chicago ablaze with my preachi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>65</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I gained 3 followers in the last week. You? Kn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>66</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>GREENSBORO,NORTH CAROLINA</td>\n",
       "      <td>How the West was burned: Thousands of wildfire...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>67</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Building the perfect tracklist to life leave t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>68</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>71</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>England.</td>\n",
       "      <td>First night with retainers in. It's quite weir...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "0    1     NaN                            NaN   \n",
       "1    4     NaN                            NaN   \n",
       "2    5     NaN                            NaN   \n",
       "3    6     NaN                            NaN   \n",
       "4    7     NaN                            NaN   \n",
       "5    8     NaN                            NaN   \n",
       "6   10     NaN                            NaN   \n",
       "7   13     NaN                            NaN   \n",
       "8   14     NaN                            NaN   \n",
       "9   15     NaN                            NaN   \n",
       "10  16     NaN                            NaN   \n",
       "11  17     NaN                            NaN   \n",
       "12  18     NaN                            NaN   \n",
       "13  19     NaN                            NaN   \n",
       "14  20     NaN                            NaN   \n",
       "15  23     NaN                            NaN   \n",
       "16  24     NaN                            NaN   \n",
       "17  25     NaN                            NaN   \n",
       "18  26     NaN                            NaN   \n",
       "19  28     NaN                            NaN   \n",
       "20  31     NaN                            NaN   \n",
       "21  32     NaN                            NaN   \n",
       "22  33     NaN                            NaN   \n",
       "23  34     NaN                            NaN   \n",
       "24  36     NaN                            NaN   \n",
       "25  37     NaN                            NaN   \n",
       "26  38     NaN                            NaN   \n",
       "27  39     NaN                            NaN   \n",
       "28  40     NaN                            NaN   \n",
       "29  41     NaN                            NaN   \n",
       "30  44     NaN                            NaN   \n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "36  54  ablaze                       Pretoria   \n",
       "37  55  ablaze                   World Wide!!   \n",
       "38  56  ablaze                            NaN   \n",
       "39  57  ablaze                 Paranaque City   \n",
       "40  59  ablaze                 Live On Webcam   \n",
       "41  61  ablaze                            NaN   \n",
       "42  62  ablaze                      milky way   \n",
       "43  63  ablaze                            NaN   \n",
       "44  64  ablaze                            NaN   \n",
       "45  65  ablaze                            NaN   \n",
       "46  66  ablaze      GREENSBORO,NORTH CAROLINA   \n",
       "47  67  ablaze                            NaN   \n",
       "48  68  ablaze                 Live On Webcam   \n",
       "49  71  ablaze                       England.   \n",
       "\n",
       "                                                 text  target  \n",
       "0   Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1              Forest fire near La Ronge Sask. Canada       1  \n",
       "2   All residents asked to 'shelter in place' are ...       1  \n",
       "3   13,000 people receive #wildfires evacuation or...       1  \n",
       "4   Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "5   #RockyFire Update => California Hwy. 20 closed...       1  \n",
       "6   #flood #disaster Heavy rain causes flash flood...       1  \n",
       "7   I'm on top of the hill and I can see a fire in...       1  \n",
       "8   There's an emergency evacuation happening now ...       1  \n",
       "9   I'm afraid that the tornado is coming to our a...       1  \n",
       "10        Three people died from the heat wave so far       1  \n",
       "11  Haha South Tampa is getting flooded hah- WAIT ...       1  \n",
       "12  #raining #flooding #Florida #TampaBay #Tampa 1...       1  \n",
       "13            #Flood in Bago Myanmar #We arrived Bago       1  \n",
       "14  Damage to school bus on 80 in multi car crash ...       1  \n",
       "15                                     What's up man?       0  \n",
       "16                                      I love fruits       0  \n",
       "17                                   Summer is lovely       0  \n",
       "18                                  My car is so fast       0  \n",
       "19                       What a goooooooaaaaaal!!!!!!       0  \n",
       "20                             this is ridiculous....       0  \n",
       "21                                  London is cool ;)       0  \n",
       "22                                        Love skiing       0  \n",
       "23                              What a wonderful day!       0  \n",
       "24                                           LOOOOOOL       0  \n",
       "25                     No way...I can't eat that shit       0  \n",
       "26                              Was in NYC last week!       0  \n",
       "27                                 Love my girlfriend       0  \n",
       "28                                          Cooool :)       0  \n",
       "29                                 Do you like pasta?       0  \n",
       "30                                           The end!       0  \n",
       "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1  \n",
       "32  We always try to bring the heavy. #metal #RT h...       0  \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1  \n",
       "34                 Crying out for more! Set me ablaze       0  \n",
       "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0  \n",
       "36  @PhDSquares #mufc they've built so much hype a...       0  \n",
       "37  INEC Office in Abia Set Ablaze - http://t.co/3...       1  \n",
       "38  Barbados #Bridgetown JAMAICA ÛÒ Two cars set ...       1  \n",
       "39                             Ablaze for you Lord :D       0  \n",
       "40  Check these out: http://t.co/rOI2NSmEJJ http:/...       0  \n",
       "41  on the outside you're ablaze and alive\\nbut yo...       0  \n",
       "42  Had an awesome time visiting the CFC head offi...       0  \n",
       "43       SOOOO PUMPED FOR ABLAZE ???? @southridgelife       0  \n",
       "44  I wanted to set Chicago ablaze with my preachi...       0  \n",
       "45  I gained 3 followers in the last week. You? Kn...       0  \n",
       "46  How the West was burned: Thousands of wildfire...       1  \n",
       "47  Building the perfect tracklist to life leave t...       0  \n",
       "48  Check these out: http://t.co/rOI2NSmEJJ http:/...       0  \n",
       "49  First night with retainers in. It's quite weir...       0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataModule(pl.LightningDataModule):\n",
    "\n",
    "  def __init__(self, train_df, test_df, tokenizer, batch_size=2, max_token_len=512):\n",
    "    super().__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.train_df = train_df\n",
    "    self.test_df = test_df\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_token_len = max_token_len\n",
    "\n",
    "  def setup(self, stage=None):\n",
    "    self.train_dataset = TwitterDataset(\n",
    "      self.train_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "\n",
    "    self.test_dataset = TwitterDataset(\n",
    "      self.test_df,\n",
    "      self.tokenizer,\n",
    "      self.max_token_len\n",
    "    )\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.train_dataset,\n",
    "      batch_size=self.batch_size,\n",
    "      shuffle=True\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size\n",
    "    )\n",
    "\n",
    "  def test_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.test_dataset,\n",
    "      batch_size=self.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "BATCH_SIZE = 2\n",
    "MAX_TOKEN_COUNT = 512\n",
    "data_module = TwitterDataModule(\n",
    "  train_df,\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClassifier(pl.LightningModule):\n",
    "\n",
    "  def __init__(self, num_labels: int, n_training_steps=None, n_warmup_steps=None, reinit_n_layers = 0):\n",
    "    super().__init__()\n",
    "\n",
    "    self.save_hyperparameters()\n",
    "    self.num_labels = num_labels\n",
    "    self.bert = AutoModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "    self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    self.n_training_steps = n_training_steps\n",
    "    self.n_warmup_steps = n_warmup_steps\n",
    "    self.criterion = nn.CrossEntropyLoss(weight = torch.tensor([0.5,1.2]))\n",
    "\n",
    "    self.reinit_n_layers = reinit_n_layers\n",
    "    if reinit_n_layers > 0: self._do_reinit()  \n",
    "\n",
    "  def _do_reinit(self):\n",
    "      # Re-init pooler.\n",
    "      self.bert.pooler.dense.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
    "      self.bert.pooler.dense.bias.data.zero_()\n",
    "      for param in self.bert.pooler.parameters():\n",
    "          param.requires_grad = True\n",
    "      \n",
    "      # Re-init last n layers.\n",
    "      for n in range(self.reinit_n_layers):\n",
    "          self.bert.encoder.layer[-(n+1)].apply(self._init_weight_and_bias)\n",
    "          \n",
    "  def _init_weight_and_bias(self, module):                        \n",
    "      if isinstance(module, nn.Linear):\n",
    "          module.weight.data.normal_(mean=0.0, std=self.bert.config.initializer_range)\n",
    "          if module.bias is not None:\n",
    "              module.bias.data.zero_()\n",
    "      elif isinstance(module, nn.LayerNorm):\n",
    "          module.bias.data.zero_()\n",
    "          module.weight.data.fill_(1.0)      \n",
    "\n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "    last_hidden_state = output.last_hidden_state\n",
    "    # As I said, the CLS token is in the beginning of the sequence. So, we grab its representation \n",
    "    # by indexing the tensor containing the hidden representations\n",
    "    CLS_token_state = last_hidden_state[:, 0, :]\n",
    "    output = self.classifier(CLS_token_state)\n",
    "           \n",
    "    loss = 0\n",
    "    if labels is not None:\n",
    "        loss = self.criterion(output, labels)\n",
    "    return loss, output\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    \n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def test_step(self, batch, batch_idx):\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    loss, outputs = self(input_ids, attention_mask, labels)\n",
    "    self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "    return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "  def training_epoch_end(self, outputs):\n",
    "    \n",
    "    softmax = nn.functional.softmax\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "        labels.append(out_labels)\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "        out_predictions = softmax(out_predictions, dim = -1)\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    class_roc_auc = auroc(predictions, labels, num_classes=self.num_labels)\n",
    "    f1_score = f1(predictions,labels,average = \"weighted\", num_classes = self.num_labels)\n",
    "    self.logger.experiment.add_scalar(f\"roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "    self.logger.experiment.add_scalar(f\"f1_weighted/Train\", f1_score, self.current_epoch)\n",
    "\n",
    "  def validation_epoch_end(self, outputs):\n",
    "    \n",
    "    softmax = nn.functional.softmax\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    for output in outputs:\n",
    "      for out_labels in output[\"labels\"].detach().cpu():\n",
    "        labels.append(out_labels)\n",
    "      for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "        out_predictions = softmax(out_predictions, dim = -1)\n",
    "        predictions.append(out_predictions)\n",
    "\n",
    "    labels = torch.stack(labels).int()\n",
    "    predictions = torch.stack(predictions)\n",
    "\n",
    "    class_roc_auc = auroc(predictions, labels, num_classes=self.num_labels)\n",
    "    f1_score = f1(predictions,labels,average = \"weighted\", num_classes = self.num_labels)\n",
    "    self.logger.experiment.add_scalar(f\"roc_auc/Valid\", class_roc_auc, self.current_epoch)\n",
    "    self.logger.experiment.add_scalar(f\"f1_weighted/Valid\", f1_score, self.current_epoch)\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "\n",
    "    optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "      optimizer,\n",
    "      num_warmup_steps=self.n_warmup_steps,\n",
    "      num_training_steps=self.n_training_steps\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "      optimizer=optimizer,\n",
    "      lr_scheduler=dict(\n",
    "        scheduler=scheduler,\n",
    "        interval='step'\n",
    "      )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 25)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TwitterClassifier(\n",
    "  num_labels=2,\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps, \n",
    "  reinit_n_layers= 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "  dirpath=\"checkpoints/twitterdisaster/\"+\"version_\"+datetime.now().strftime(\"%d-%m-%Y--%H-%M\"),\n",
    "  filename=\"best-checkpoint\",\n",
    "  save_top_k=1,\n",
    "  verbose=True,\n",
    "  monitor=\"val_loss\",\n",
    "  mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(\"lightning_logs\", version =\"version_\"+datetime.now().strftime(\"%d-%m-%Y--%H-%M\") ,name=\"twitter-disaster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "C:\\Users\\ntaylor\\AppData\\Roaming\\Python\\Python38\\site-packages\\pytorch_lightning\\utilities\\distributed.py:68: UserWarning: GPU available but not used. Set the --gpus flag when calling the script.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "  logger=logger,\n",
    "  checkpoint_callback=checkpoint_callback,\n",
    "  callbacks=[early_stopping_callback],\n",
    "  max_epochs=N_EPOCHS ,\n",
    "  gpus=None, \n",
    "  accumulate_grad_batches=4\n",
    "  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | bert       | BertModel        | 109 M \n",
      "1 | classifier | Linear           | 1.5 K \n",
      "2 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.935   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 35/35 [02:33<00:00,  4.39s/it, loss=0.258, v_num=3-47, val_loss=0.329, train_loss=0.269]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 6: val_loss reached 0.32927 (best 0.32927), saving model to \"C:\\Users\\ntaylor\\Documents\\GitHub\\Neural_Networks\\NLP_Mimic_only\\pytorch-lightning-models\\checkpoints\\twitterdisaster\\version_11-11-2021--13-47\\best-checkpoint.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 35/35 [03:07<00:00,  5.37s/it, loss=0.258, v_num=3-47, val_loss=0.329, train_loss=0.269]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.22611144185066223}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.22611144185066223}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(ckpt_path=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"./checkpoints/twitterdisaster/\"\n",
    "trained_model = TwitterClassifier.load_from_checkpoint(\n",
    "  f\"{ckpt_dir}best-checkpoint-v4.ckpt\", map_location=\"cpu\"\n",
    ")\n",
    "trained_model.eval()\n",
    "trained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real: 0.532301664352417\n",
      "fake: 0.4676983654499054\n"
     ]
    }
   ],
   "source": [
    "test_comment = \"Wind blowing. Danger coming\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(\n",
    "  test_comment,\n",
    "  add_special_tokens=True,\n",
    "  max_length=512,\n",
    "  return_token_type_ids=False,\n",
    "  padding=\"max_length\",\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "_, test_prediction = trained_model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "test_prediction = nn.functional.softmax(test_prediction, dim = 1)\n",
    "test_prediction = test_prediction.flatten().numpy()\n",
    "np.argmax(test_prediction, axis = 0)\n",
    "LABEL_COLUMNS = [\"real\",\"fake\"]\n",
    "for label, prediction in zip(LABEL_COLUMNS, test_prediction):\n",
    "  print(f\"{label}: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:01<00:00, 15.99it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "val_dataset = TwitterDataset(\n",
    "  val_df,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_TOKEN_COUNT\n",
    ")\n",
    "\n",
    "predictions = []\n",
    "labels = []\n",
    "\n",
    "for item in tqdm(val_dataset):\n",
    "  _, prediction = trained_model(\n",
    "    item[\"input_ids\"].unsqueeze(dim=0).to(device), \n",
    "    item[\"attention_mask\"].unsqueeze(dim=0).to(device)\n",
    "  )\n",
    "  prediction = nn.functional.softmax(prediction, dim = 1)\n",
    "  predictions.append(prediction.flatten())\n",
    "  labels.append(item[\"labels\"].int())\n",
    "\n",
    "predictions = torch.stack(predictions).detach().cpu()\n",
    "labels = torch.stack(labels).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6316)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1],\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7667)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst_auroc = auroc(predictions, labels, num_classes = 2)\n",
    "inst_auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19, 2])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        real       0.71      0.50      0.59        10\n",
      "        fake       0.58      0.78      0.67         9\n",
      "\n",
      "    accuracy                           0.63        19\n",
      "   macro avg       0.65      0.64      0.63        19\n",
      "weighted avg       0.65      0.63      0.63        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictions.numpy()\n",
    "y_true = labels.numpy()\n",
    "\n",
    "upper, lower = 1, 0\n",
    "\n",
    "y_pred = np.argmax(y_pred,axis=1)\n",
    "\n",
    "print(classification_report(\n",
    "  y_true, \n",
    "  y_pred, \n",
    "  target_names=[\"real\",\"fake\"], \n",
    "  zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1924dc1dd04b7e09b7d5eb58cba0f493e2fde5c6da36fb26502589a8e5856a6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
