{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8c3e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from openprompt.data_utils import PROCESSORS\n",
    "import torch\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer, ManualTemplate, SoftVerbalizer\n",
    "\n",
    "from openprompt.prompts import SoftTemplate\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "from openprompt.plms.seq2seq import T5TokenizerWrapper, T5LMTokenizerWrapper\n",
    "from transformers import T5Config, T5Tokenizer, T5ForConditionalGeneration\n",
    "from openprompt.data_utils.data_sampler import FewShotSampler\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "from utils import Mimic_ICD9_Processor, Mimic_ICD9_Triage_Processor\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from loguru import logger\n",
    "\n",
    "import torchmetrics.functional.classification as metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996d15e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"./checkpoints/icd9_50/emilyalsentzer/Bio_ClinicalBERT_tempmanual2_verbsoft0/version_21-01-2022--13-41/checkpoint.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f201fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load(ckpt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7292dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['plm', 'template', 'verbalizer'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0306a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "state_dict() missing 1 required positional argument: 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bec79c1bb7cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPromptForClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: state_dict() missing 1 required positional argument: 'self'"
     ]
    }
   ],
   "source": [
    "PromptForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original way of instantiating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec56dbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can load the trained state dict for the plm like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82065d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba73c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ace9cb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35af0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template\n",
    "\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(\"scripts/mimic_icd9_top50/manual_template.txt\", choice=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97efa9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft template\n",
    "\n",
    "soft_template = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=20, initialize_from_vocab=True).from_file(f\"scripts/mimic_icd9_top50/soft_template.txt\", choice=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4115c7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftTemplate(\n",
       "  (raw_embedding): Embedding(28996, 768, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "112dc64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('[PAD]', 0),\n",
       "             ('[unused1]', 1),\n",
       "             ('[unused2]', 2),\n",
       "             ('[unused3]', 3),\n",
       "             ('[unused4]', 4),\n",
       "             ('[unused5]', 5),\n",
       "             ('[unused6]', 6),\n",
       "             ('[unused7]', 7),\n",
       "             ('[unused8]', 8),\n",
       "             ('[unused9]', 9),\n",
       "             ('[unused10]', 10),\n",
       "             ('[unused11]', 11),\n",
       "             ('[unused12]', 12),\n",
       "             ('[unused13]', 13),\n",
       "             ('[unused14]', 14),\n",
       "             ('[unused15]', 15),\n",
       "             ('[unused16]', 16),\n",
       "             ('[unused17]', 17),\n",
       "             ('[unused18]', 18),\n",
       "             ('[unused19]', 19),\n",
       "             ('[unused20]', 20),\n",
       "             ('[unused21]', 21),\n",
       "             ('[unused22]', 22),\n",
       "             ('[unused23]', 23),\n",
       "             ('[unused24]', 24),\n",
       "             ('[unused25]', 25),\n",
       "             ('[unused26]', 26),\n",
       "             ('[unused27]', 27),\n",
       "             ('[unused28]', 28),\n",
       "             ('[unused29]', 29),\n",
       "             ('[unused30]', 30),\n",
       "             ('[unused31]', 31),\n",
       "             ('[unused32]', 32),\n",
       "             ('[unused33]', 33),\n",
       "             ('[unused34]', 34),\n",
       "             ('[unused35]', 35),\n",
       "             ('[unused36]', 36),\n",
       "             ('[unused37]', 37),\n",
       "             ('[unused38]', 38),\n",
       "             ('[unused39]', 39),\n",
       "             ('[unused40]', 40),\n",
       "             ('[unused41]', 41),\n",
       "             ('[unused42]', 42),\n",
       "             ('[unused43]', 43),\n",
       "             ('[unused44]', 44),\n",
       "             ('[unused45]', 45),\n",
       "             ('[unused46]', 46),\n",
       "             ('[unused47]', 47),\n",
       "             ('[unused48]', 48),\n",
       "             ('[unused49]', 49),\n",
       "             ('[unused50]', 50),\n",
       "             ('[unused51]', 51),\n",
       "             ('[unused52]', 52),\n",
       "             ('[unused53]', 53),\n",
       "             ('[unused54]', 54),\n",
       "             ('[unused55]', 55),\n",
       "             ('[unused56]', 56),\n",
       "             ('[unused57]', 57),\n",
       "             ('[unused58]', 58),\n",
       "             ('[unused59]', 59),\n",
       "             ('[unused60]', 60),\n",
       "             ('[unused61]', 61),\n",
       "             ('[unused62]', 62),\n",
       "             ('[unused63]', 63),\n",
       "             ('[unused64]', 64),\n",
       "             ('[unused65]', 65),\n",
       "             ('[unused66]', 66),\n",
       "             ('[unused67]', 67),\n",
       "             ('[unused68]', 68),\n",
       "             ('[unused69]', 69),\n",
       "             ('[unused70]', 70),\n",
       "             ('[unused71]', 71),\n",
       "             ('[unused72]', 72),\n",
       "             ('[unused73]', 73),\n",
       "             ('[unused74]', 74),\n",
       "             ('[unused75]', 75),\n",
       "             ('[unused76]', 76),\n",
       "             ('[unused77]', 77),\n",
       "             ('[unused78]', 78),\n",
       "             ('[unused79]', 79),\n",
       "             ('[unused80]', 80),\n",
       "             ('[unused81]', 81),\n",
       "             ('[unused82]', 82),\n",
       "             ('[unused83]', 83),\n",
       "             ('[unused84]', 84),\n",
       "             ('[unused85]', 85),\n",
       "             ('[unused86]', 86),\n",
       "             ('[unused87]', 87),\n",
       "             ('[unused88]', 88),\n",
       "             ('[unused89]', 89),\n",
       "             ('[unused90]', 90),\n",
       "             ('[unused91]', 91),\n",
       "             ('[unused92]', 92),\n",
       "             ('[unused93]', 93),\n",
       "             ('[unused94]', 94),\n",
       "             ('[unused95]', 95),\n",
       "             ('[unused96]', 96),\n",
       "             ('[unused97]', 97),\n",
       "             ('[unused98]', 98),\n",
       "             ('[unused99]', 99),\n",
       "             ('[UNK]', 100),\n",
       "             ('[CLS]', 101),\n",
       "             ('[SEP]', 102),\n",
       "             ('[MASK]', 103),\n",
       "             ('[unused100]', 104),\n",
       "             ('[unused101]', 105),\n",
       "             ('!', 106),\n",
       "             ('\"', 107),\n",
       "             ('#', 108),\n",
       "             ('$', 109),\n",
       "             ('%', 110),\n",
       "             ('&', 111),\n",
       "             (\"'\", 112),\n",
       "             ('(', 113),\n",
       "             (')', 114),\n",
       "             ('*', 115),\n",
       "             ('+', 116),\n",
       "             (',', 117),\n",
       "             ('-', 118),\n",
       "             ('.', 119),\n",
       "             ('/', 120),\n",
       "             ('0', 121),\n",
       "             ('1', 122),\n",
       "             ('2', 123),\n",
       "             ('3', 124),\n",
       "             ('4', 125),\n",
       "             ('5', 126),\n",
       "             ('6', 127),\n",
       "             ('7', 128),\n",
       "             ('8', 129),\n",
       "             ('9', 130),\n",
       "             (':', 131),\n",
       "             (';', 132),\n",
       "             ('<', 133),\n",
       "             ('=', 134),\n",
       "             ('>', 135),\n",
       "             ('?', 136),\n",
       "             ('@', 137),\n",
       "             ('A', 138),\n",
       "             ('B', 139),\n",
       "             ('C', 140),\n",
       "             ('D', 141),\n",
       "             ('E', 142),\n",
       "             ('F', 143),\n",
       "             ('G', 144),\n",
       "             ('H', 145),\n",
       "             ('I', 146),\n",
       "             ('J', 147),\n",
       "             ('K', 148),\n",
       "             ('L', 149),\n",
       "             ('M', 150),\n",
       "             ('N', 151),\n",
       "             ('O', 152),\n",
       "             ('P', 153),\n",
       "             ('Q', 154),\n",
       "             ('R', 155),\n",
       "             ('S', 156),\n",
       "             ('T', 157),\n",
       "             ('U', 158),\n",
       "             ('V', 159),\n",
       "             ('W', 160),\n",
       "             ('X', 161),\n",
       "             ('Y', 162),\n",
       "             ('Z', 163),\n",
       "             ('[', 164),\n",
       "             ('\\\\', 165),\n",
       "             (']', 166),\n",
       "             ('^', 167),\n",
       "             ('_', 168),\n",
       "             ('`', 169),\n",
       "             ('a', 170),\n",
       "             ('b', 171),\n",
       "             ('c', 172),\n",
       "             ('d', 173),\n",
       "             ('e', 174),\n",
       "             ('f', 175),\n",
       "             ('g', 176),\n",
       "             ('h', 177),\n",
       "             ('i', 178),\n",
       "             ('j', 179),\n",
       "             ('k', 180),\n",
       "             ('l', 181),\n",
       "             ('m', 182),\n",
       "             ('n', 183),\n",
       "             ('o', 184),\n",
       "             ('p', 185),\n",
       "             ('q', 186),\n",
       "             ('r', 187),\n",
       "             ('s', 188),\n",
       "             ('t', 189),\n",
       "             ('u', 190),\n",
       "             ('v', 191),\n",
       "             ('w', 192),\n",
       "             ('x', 193),\n",
       "             ('y', 194),\n",
       "             ('z', 195),\n",
       "             ('{', 196),\n",
       "             ('|', 197),\n",
       "             ('}', 198),\n",
       "             ('~', 199),\n",
       "             ('¡', 200),\n",
       "             ('¢', 201),\n",
       "             ('£', 202),\n",
       "             ('¥', 203),\n",
       "             ('§', 204),\n",
       "             ('¨', 205),\n",
       "             ('©', 206),\n",
       "             ('ª', 207),\n",
       "             ('«', 208),\n",
       "             ('¬', 209),\n",
       "             ('®', 210),\n",
       "             ('°', 211),\n",
       "             ('±', 212),\n",
       "             ('²', 213),\n",
       "             ('³', 214),\n",
       "             ('´', 215),\n",
       "             ('µ', 216),\n",
       "             ('¶', 217),\n",
       "             ('·', 218),\n",
       "             ('¹', 219),\n",
       "             ('º', 220),\n",
       "             ('»', 221),\n",
       "             ('¼', 222),\n",
       "             ('½', 223),\n",
       "             ('¾', 224),\n",
       "             ('¿', 225),\n",
       "             ('À', 226),\n",
       "             ('Á', 227),\n",
       "             ('Â', 228),\n",
       "             ('Ä', 229),\n",
       "             ('Å', 230),\n",
       "             ('Æ', 231),\n",
       "             ('Ç', 232),\n",
       "             ('È', 233),\n",
       "             ('É', 234),\n",
       "             ('Í', 235),\n",
       "             ('Î', 236),\n",
       "             ('Ñ', 237),\n",
       "             ('Ó', 238),\n",
       "             ('Ö', 239),\n",
       "             ('×', 240),\n",
       "             ('Ø', 241),\n",
       "             ('Ú', 242),\n",
       "             ('Ü', 243),\n",
       "             ('Þ', 244),\n",
       "             ('ß', 245),\n",
       "             ('à', 246),\n",
       "             ('á', 247),\n",
       "             ('â', 248),\n",
       "             ('ã', 249),\n",
       "             ('ä', 250),\n",
       "             ('å', 251),\n",
       "             ('æ', 252),\n",
       "             ('ç', 253),\n",
       "             ('è', 254),\n",
       "             ('é', 255),\n",
       "             ('ê', 256),\n",
       "             ('ë', 257),\n",
       "             ('ì', 258),\n",
       "             ('í', 259),\n",
       "             ('î', 260),\n",
       "             ('ï', 261),\n",
       "             ('ð', 262),\n",
       "             ('ñ', 263),\n",
       "             ('ò', 264),\n",
       "             ('ó', 265),\n",
       "             ('ô', 266),\n",
       "             ('õ', 267),\n",
       "             ('ö', 268),\n",
       "             ('÷', 269),\n",
       "             ('ø', 270),\n",
       "             ('ù', 271),\n",
       "             ('ú', 272),\n",
       "             ('û', 273),\n",
       "             ('ü', 274),\n",
       "             ('ý', 275),\n",
       "             ('þ', 276),\n",
       "             ('ÿ', 277),\n",
       "             ('Ā', 278),\n",
       "             ('ā', 279),\n",
       "             ('ă', 280),\n",
       "             ('ą', 281),\n",
       "             ('Ć', 282),\n",
       "             ('ć', 283),\n",
       "             ('Č', 284),\n",
       "             ('č', 285),\n",
       "             ('ď', 286),\n",
       "             ('Đ', 287),\n",
       "             ('đ', 288),\n",
       "             ('ē', 289),\n",
       "             ('ė', 290),\n",
       "             ('ę', 291),\n",
       "             ('ě', 292),\n",
       "             ('ğ', 293),\n",
       "             ('ġ', 294),\n",
       "             ('Ħ', 295),\n",
       "             ('ħ', 296),\n",
       "             ('ĩ', 297),\n",
       "             ('Ī', 298),\n",
       "             ('ī', 299),\n",
       "             ('İ', 300),\n",
       "             ('ı', 301),\n",
       "             ('ļ', 302),\n",
       "             ('Ľ', 303),\n",
       "             ('ľ', 304),\n",
       "             ('Ł', 305),\n",
       "             ('ł', 306),\n",
       "             ('ń', 307),\n",
       "             ('ņ', 308),\n",
       "             ('ň', 309),\n",
       "             ('ŋ', 310),\n",
       "             ('Ō', 311),\n",
       "             ('ō', 312),\n",
       "             ('ŏ', 313),\n",
       "             ('ő', 314),\n",
       "             ('Œ', 315),\n",
       "             ('œ', 316),\n",
       "             ('ř', 317),\n",
       "             ('Ś', 318),\n",
       "             ('ś', 319),\n",
       "             ('Ş', 320),\n",
       "             ('ş', 321),\n",
       "             ('Š', 322),\n",
       "             ('š', 323),\n",
       "             ('Ţ', 324),\n",
       "             ('ţ', 325),\n",
       "             ('ť', 326),\n",
       "             ('ũ', 327),\n",
       "             ('ū', 328),\n",
       "             ('ŭ', 329),\n",
       "             ('ů', 330),\n",
       "             ('ű', 331),\n",
       "             ('ų', 332),\n",
       "             ('ŵ', 333),\n",
       "             ('ŷ', 334),\n",
       "             ('ź', 335),\n",
       "             ('Ż', 336),\n",
       "             ('ż', 337),\n",
       "             ('Ž', 338),\n",
       "             ('ž', 339),\n",
       "             ('Ə', 340),\n",
       "             ('ƒ', 341),\n",
       "             ('ơ', 342),\n",
       "             ('ư', 343),\n",
       "             ('ǎ', 344),\n",
       "             ('ǐ', 345),\n",
       "             ('ǒ', 346),\n",
       "             ('ǔ', 347),\n",
       "             ('ǫ', 348),\n",
       "             ('Ș', 349),\n",
       "             ('ș', 350),\n",
       "             ('Ț', 351),\n",
       "             ('ț', 352),\n",
       "             ('ɐ', 353),\n",
       "             ('ɑ', 354),\n",
       "             ('ɔ', 355),\n",
       "             ('ɕ', 356),\n",
       "             ('ə', 357),\n",
       "             ('ɛ', 358),\n",
       "             ('ɡ', 359),\n",
       "             ('ɣ', 360),\n",
       "             ('ɨ', 361),\n",
       "             ('ɪ', 362),\n",
       "             ('ɲ', 363),\n",
       "             ('ɾ', 364),\n",
       "             ('ʀ', 365),\n",
       "             ('ʁ', 366),\n",
       "             ('ʂ', 367),\n",
       "             ('ʃ', 368),\n",
       "             ('ʊ', 369),\n",
       "             ('ʋ', 370),\n",
       "             ('ʌ', 371),\n",
       "             ('ʐ', 372),\n",
       "             ('ʑ', 373),\n",
       "             ('ʒ', 374),\n",
       "             ('ʔ', 375),\n",
       "             ('ʰ', 376),\n",
       "             ('ʲ', 377),\n",
       "             ('ʳ', 378),\n",
       "             ('ʷ', 379),\n",
       "             ('ʻ', 380),\n",
       "             ('ʼ', 381),\n",
       "             ('ʾ', 382),\n",
       "             ('ʿ', 383),\n",
       "             ('ˈ', 384),\n",
       "             ('ː', 385),\n",
       "             ('ˡ', 386),\n",
       "             ('ˢ', 387),\n",
       "             ('ˣ', 388),\n",
       "             ('́', 389),\n",
       "             ('̃', 390),\n",
       "             ('̍', 391),\n",
       "             ('̯', 392),\n",
       "             ('͡', 393),\n",
       "             ('Α', 394),\n",
       "             ('Β', 395),\n",
       "             ('Γ', 396),\n",
       "             ('Δ', 397),\n",
       "             ('Ε', 398),\n",
       "             ('Η', 399),\n",
       "             ('Θ', 400),\n",
       "             ('Ι', 401),\n",
       "             ('Κ', 402),\n",
       "             ('Λ', 403),\n",
       "             ('Μ', 404),\n",
       "             ('Ν', 405),\n",
       "             ('Ο', 406),\n",
       "             ('Π', 407),\n",
       "             ('Σ', 408),\n",
       "             ('Τ', 409),\n",
       "             ('Φ', 410),\n",
       "             ('Χ', 411),\n",
       "             ('Ψ', 412),\n",
       "             ('Ω', 413),\n",
       "             ('ά', 414),\n",
       "             ('έ', 415),\n",
       "             ('ή', 416),\n",
       "             ('ί', 417),\n",
       "             ('α', 418),\n",
       "             ('β', 419),\n",
       "             ('γ', 420),\n",
       "             ('δ', 421),\n",
       "             ('ε', 422),\n",
       "             ('ζ', 423),\n",
       "             ('η', 424),\n",
       "             ('θ', 425),\n",
       "             ('ι', 426),\n",
       "             ('κ', 427),\n",
       "             ('λ', 428),\n",
       "             ('μ', 429),\n",
       "             ('ν', 430),\n",
       "             ('ξ', 431),\n",
       "             ('ο', 432),\n",
       "             ('π', 433),\n",
       "             ('ρ', 434),\n",
       "             ('ς', 435),\n",
       "             ('σ', 436),\n",
       "             ('τ', 437),\n",
       "             ('υ', 438),\n",
       "             ('φ', 439),\n",
       "             ('χ', 440),\n",
       "             ('ψ', 441),\n",
       "             ('ω', 442),\n",
       "             ('ό', 443),\n",
       "             ('ύ', 444),\n",
       "             ('ώ', 445),\n",
       "             ('І', 446),\n",
       "             ('Ј', 447),\n",
       "             ('А', 448),\n",
       "             ('Б', 449),\n",
       "             ('В', 450),\n",
       "             ('Г', 451),\n",
       "             ('Д', 452),\n",
       "             ('Е', 453),\n",
       "             ('Ж', 454),\n",
       "             ('З', 455),\n",
       "             ('И', 456),\n",
       "             ('К', 457),\n",
       "             ('Л', 458),\n",
       "             ('М', 459),\n",
       "             ('Н', 460),\n",
       "             ('О', 461),\n",
       "             ('П', 462),\n",
       "             ('Р', 463),\n",
       "             ('С', 464),\n",
       "             ('Т', 465),\n",
       "             ('У', 466),\n",
       "             ('Ф', 467),\n",
       "             ('Х', 468),\n",
       "             ('Ц', 469),\n",
       "             ('Ч', 470),\n",
       "             ('Ш', 471),\n",
       "             ('Э', 472),\n",
       "             ('Ю', 473),\n",
       "             ('Я', 474),\n",
       "             ('а', 475),\n",
       "             ('б', 476),\n",
       "             ('в', 477),\n",
       "             ('г', 478),\n",
       "             ('д', 479),\n",
       "             ('е', 480),\n",
       "             ('ж', 481),\n",
       "             ('з', 482),\n",
       "             ('и', 483),\n",
       "             ('й', 484),\n",
       "             ('к', 485),\n",
       "             ('л', 486),\n",
       "             ('м', 487),\n",
       "             ('н', 488),\n",
       "             ('о', 489),\n",
       "             ('п', 490),\n",
       "             ('р', 491),\n",
       "             ('с', 492),\n",
       "             ('т', 493),\n",
       "             ('у', 494),\n",
       "             ('ф', 495),\n",
       "             ('х', 496),\n",
       "             ('ц', 497),\n",
       "             ('ч', 498),\n",
       "             ('ш', 499),\n",
       "             ('щ', 500),\n",
       "             ('ъ', 501),\n",
       "             ('ы', 502),\n",
       "             ('ь', 503),\n",
       "             ('э', 504),\n",
       "             ('ю', 505),\n",
       "             ('я', 506),\n",
       "             ('ё', 507),\n",
       "             ('і', 508),\n",
       "             ('ї', 509),\n",
       "             ('ј', 510),\n",
       "             ('њ', 511),\n",
       "             ('ћ', 512),\n",
       "             ('Ա', 513),\n",
       "             ('Հ', 514),\n",
       "             ('ա', 515),\n",
       "             ('ե', 516),\n",
       "             ('ի', 517),\n",
       "             ('կ', 518),\n",
       "             ('մ', 519),\n",
       "             ('յ', 520),\n",
       "             ('ն', 521),\n",
       "             ('ո', 522),\n",
       "             ('ս', 523),\n",
       "             ('տ', 524),\n",
       "             ('ր', 525),\n",
       "             ('ւ', 526),\n",
       "             ('ְ', 527),\n",
       "             ('ִ', 528),\n",
       "             ('ֵ', 529),\n",
       "             ('ֶ', 530),\n",
       "             ('ַ', 531),\n",
       "             ('ָ', 532),\n",
       "             ('ֹ', 533),\n",
       "             ('ּ', 534),\n",
       "             ('א', 535),\n",
       "             ('ב', 536),\n",
       "             ('ג', 537),\n",
       "             ('ד', 538),\n",
       "             ('ה', 539),\n",
       "             ('ו', 540),\n",
       "             ('ז', 541),\n",
       "             ('ח', 542),\n",
       "             ('ט', 543),\n",
       "             ('י', 544),\n",
       "             ('כ', 545),\n",
       "             ('ל', 546),\n",
       "             ('ם', 547),\n",
       "             ('מ', 548),\n",
       "             ('ן', 549),\n",
       "             ('נ', 550),\n",
       "             ('ס', 551),\n",
       "             ('ע', 552),\n",
       "             ('פ', 553),\n",
       "             ('צ', 554),\n",
       "             ('ק', 555),\n",
       "             ('ר', 556),\n",
       "             ('ש', 557),\n",
       "             ('ת', 558),\n",
       "             ('،', 559),\n",
       "             ('ء', 560),\n",
       "             ('آ', 561),\n",
       "             ('أ', 562),\n",
       "             ('إ', 563),\n",
       "             ('ئ', 564),\n",
       "             ('ا', 565),\n",
       "             ('ب', 566),\n",
       "             ('ة', 567),\n",
       "             ('ت', 568),\n",
       "             ('ث', 569),\n",
       "             ('ج', 570),\n",
       "             ('ح', 571),\n",
       "             ('خ', 572),\n",
       "             ('د', 573),\n",
       "             ('ذ', 574),\n",
       "             ('ر', 575),\n",
       "             ('ز', 576),\n",
       "             ('س', 577),\n",
       "             ('ش', 578),\n",
       "             ('ص', 579),\n",
       "             ('ض', 580),\n",
       "             ('ط', 581),\n",
       "             ('ظ', 582),\n",
       "             ('ع', 583),\n",
       "             ('غ', 584),\n",
       "             ('ف', 585),\n",
       "             ('ق', 586),\n",
       "             ('ك', 587),\n",
       "             ('ل', 588),\n",
       "             ('م', 589),\n",
       "             ('ن', 590),\n",
       "             ('ه', 591),\n",
       "             ('و', 592),\n",
       "             ('ى', 593),\n",
       "             ('ي', 594),\n",
       "             ('َ', 595),\n",
       "             ('ِ', 596),\n",
       "             ('ٹ', 597),\n",
       "             ('پ', 598),\n",
       "             ('چ', 599),\n",
       "             ('ک', 600),\n",
       "             ('گ', 601),\n",
       "             ('ہ', 602),\n",
       "             ('ی', 603),\n",
       "             ('ے', 604),\n",
       "             ('ं', 605),\n",
       "             ('आ', 606),\n",
       "             ('क', 607),\n",
       "             ('ग', 608),\n",
       "             ('च', 609),\n",
       "             ('ज', 610),\n",
       "             ('ण', 611),\n",
       "             ('त', 612),\n",
       "             ('द', 613),\n",
       "             ('ध', 614),\n",
       "             ('न', 615),\n",
       "             ('प', 616),\n",
       "             ('ब', 617),\n",
       "             ('भ', 618),\n",
       "             ('म', 619),\n",
       "             ('य', 620),\n",
       "             ('र', 621),\n",
       "             ('ल', 622),\n",
       "             ('व', 623),\n",
       "             ('श', 624),\n",
       "             ('ष', 625),\n",
       "             ('स', 626),\n",
       "             ('ह', 627),\n",
       "             ('ा', 628),\n",
       "             ('ि', 629),\n",
       "             ('ी', 630),\n",
       "             ('ु', 631),\n",
       "             ('े', 632),\n",
       "             ('ो', 633),\n",
       "             ('्', 634),\n",
       "             ('।', 635),\n",
       "             ('॥', 636),\n",
       "             ('আ', 637),\n",
       "             ('ই', 638),\n",
       "             ('এ', 639),\n",
       "             ('ও', 640),\n",
       "             ('ক', 641),\n",
       "             ('খ', 642),\n",
       "             ('গ', 643),\n",
       "             ('চ', 644),\n",
       "             ('ছ', 645),\n",
       "             ('জ', 646),\n",
       "             ('ট', 647),\n",
       "             ('ত', 648),\n",
       "             ('থ', 649),\n",
       "             ('দ', 650),\n",
       "             ('ধ', 651),\n",
       "             ('ন', 652),\n",
       "             ('প', 653),\n",
       "             ('ব', 654),\n",
       "             ('ম', 655),\n",
       "             ('য', 656),\n",
       "             ('র', 657),\n",
       "             ('ল', 658),\n",
       "             ('শ', 659),\n",
       "             ('স', 660),\n",
       "             ('হ', 661),\n",
       "             ('়', 662),\n",
       "             ('া', 663),\n",
       "             ('ি', 664),\n",
       "             ('ী', 665),\n",
       "             ('ু', 666),\n",
       "             ('ে', 667),\n",
       "             ('ো', 668),\n",
       "             ('্', 669),\n",
       "             ('য়', 670),\n",
       "             ('க', 671),\n",
       "             ('த', 672),\n",
       "             ('ப', 673),\n",
       "             ('ம', 674),\n",
       "             ('ய', 675),\n",
       "             ('ர', 676),\n",
       "             ('ல', 677),\n",
       "             ('வ', 678),\n",
       "             ('ா', 679),\n",
       "             ('ி', 680),\n",
       "             ('ு', 681),\n",
       "             ('்', 682),\n",
       "             ('ร', 683),\n",
       "             ('་', 684),\n",
       "             ('ག', 685),\n",
       "             ('ང', 686),\n",
       "             ('ད', 687),\n",
       "             ('ན', 688),\n",
       "             ('བ', 689),\n",
       "             ('མ', 690),\n",
       "             ('ར', 691),\n",
       "             ('ལ', 692),\n",
       "             ('ས', 693),\n",
       "             ('ི', 694),\n",
       "             ('ུ', 695),\n",
       "             ('ེ', 696),\n",
       "             ('ོ', 697),\n",
       "             ('ა', 698),\n",
       "             ('ე', 699),\n",
       "             ('ი', 700),\n",
       "             ('ლ', 701),\n",
       "             ('ნ', 702),\n",
       "             ('ო', 703),\n",
       "             ('რ', 704),\n",
       "             ('ს', 705),\n",
       "             ('ᴬ', 706),\n",
       "             ('ᴵ', 707),\n",
       "             ('ᵀ', 708),\n",
       "             ('ᵃ', 709),\n",
       "             ('ᵇ', 710),\n",
       "             ('ᵈ', 711),\n",
       "             ('ᵉ', 712),\n",
       "             ('ᵍ', 713),\n",
       "             ('ᵏ', 714),\n",
       "             ('ᵐ', 715),\n",
       "             ('ᵒ', 716),\n",
       "             ('ᵖ', 717),\n",
       "             ('ᵗ', 718),\n",
       "             ('ᵘ', 719),\n",
       "             ('ᵢ', 720),\n",
       "             ('ᵣ', 721),\n",
       "             ('ᵤ', 722),\n",
       "             ('ᵥ', 723),\n",
       "             ('ᶜ', 724),\n",
       "             ('ᶠ', 725),\n",
       "             ('ḍ', 726),\n",
       "             ('Ḥ', 727),\n",
       "             ('ḥ', 728),\n",
       "             ('Ḩ', 729),\n",
       "             ('ḩ', 730),\n",
       "             ('ḳ', 731),\n",
       "             ('ṃ', 732),\n",
       "             ('ṅ', 733),\n",
       "             ('ṇ', 734),\n",
       "             ('ṛ', 735),\n",
       "             ('ṣ', 736),\n",
       "             ('ṭ', 737),\n",
       "             ('ạ', 738),\n",
       "             ('ả', 739),\n",
       "             ('ấ', 740),\n",
       "             ('ầ', 741),\n",
       "             ('ẩ', 742),\n",
       "             ('ậ', 743),\n",
       "             ('ắ', 744),\n",
       "             ('ế', 745),\n",
       "             ('ề', 746),\n",
       "             ('ể', 747),\n",
       "             ('ễ', 748),\n",
       "             ('ệ', 749),\n",
       "             ('ị', 750),\n",
       "             ('ọ', 751),\n",
       "             ('ố', 752),\n",
       "             ('ồ', 753),\n",
       "             ('ổ', 754),\n",
       "             ('ộ', 755),\n",
       "             ('ớ', 756),\n",
       "             ('ờ', 757),\n",
       "             ('ợ', 758),\n",
       "             ('ụ', 759),\n",
       "             ('ủ', 760),\n",
       "             ('ứ', 761),\n",
       "             ('ừ', 762),\n",
       "             ('ử', 763),\n",
       "             ('ữ', 764),\n",
       "             ('ự', 765),\n",
       "             ('ỳ', 766),\n",
       "             ('ỹ', 767),\n",
       "             ('ἀ', 768),\n",
       "             ('ἐ', 769),\n",
       "             ('ὁ', 770),\n",
       "             ('ὐ', 771),\n",
       "             ('ὰ', 772),\n",
       "             ('ὶ', 773),\n",
       "             ('ὸ', 774),\n",
       "             ('ῆ', 775),\n",
       "             ('ῖ', 776),\n",
       "             ('ῦ', 777),\n",
       "             ('ῶ', 778),\n",
       "             ('‐', 779),\n",
       "             ('‑', 780),\n",
       "             ('‒', 781),\n",
       "             ('–', 782),\n",
       "             ('—', 783),\n",
       "             ('―', 784),\n",
       "             ('‖', 785),\n",
       "             ('‘', 786),\n",
       "             ('’', 787),\n",
       "             ('‚', 788),\n",
       "             ('“', 789),\n",
       "             ('”', 790),\n",
       "             ('„', 791),\n",
       "             ('†', 792),\n",
       "             ('‡', 793),\n",
       "             ('•', 794),\n",
       "             ('…', 795),\n",
       "             ('‰', 796),\n",
       "             ('′', 797),\n",
       "             ('″', 798),\n",
       "             ('⁄', 799),\n",
       "             ('⁰', 800),\n",
       "             ('ⁱ', 801),\n",
       "             ('⁴', 802),\n",
       "             ('⁵', 803),\n",
       "             ('⁶', 804),\n",
       "             ('⁷', 805),\n",
       "             ('⁸', 806),\n",
       "             ('⁹', 807),\n",
       "             ('⁺', 808),\n",
       "             ('⁻', 809),\n",
       "             ('ⁿ', 810),\n",
       "             ('₀', 811),\n",
       "             ('₁', 812),\n",
       "             ('₂', 813),\n",
       "             ('₃', 814),\n",
       "             ('₄', 815),\n",
       "             ('₅', 816),\n",
       "             ('₆', 817),\n",
       "             ('₇', 818),\n",
       "             ('₈', 819),\n",
       "             ('₉', 820),\n",
       "             ('₊', 821),\n",
       "             ('₍', 822),\n",
       "             ('₎', 823),\n",
       "             ('ₐ', 824),\n",
       "             ('ₑ', 825),\n",
       "             ('ₒ', 826),\n",
       "             ('ₓ', 827),\n",
       "             ('ₕ', 828),\n",
       "             ('ₖ', 829),\n",
       "             ('ₘ', 830),\n",
       "             ('ₙ', 831),\n",
       "             ('ₚ', 832),\n",
       "             ('ₛ', 833),\n",
       "             ('ₜ', 834),\n",
       "             ('₤', 835),\n",
       "             ('€', 836),\n",
       "             ('₱', 837),\n",
       "             ('₹', 838),\n",
       "             ('ℓ', 839),\n",
       "             ('№', 840),\n",
       "             ('ℝ', 841),\n",
       "             ('⅓', 842),\n",
       "             ('←', 843),\n",
       "             ('↑', 844),\n",
       "             ('→', 845),\n",
       "             ('↔', 846),\n",
       "             ('⇌', 847),\n",
       "             ('⇒', 848),\n",
       "             ('∂', 849),\n",
       "             ('∈', 850),\n",
       "             ('−', 851),\n",
       "             ('∗', 852),\n",
       "             ('∘', 853),\n",
       "             ('√', 854),\n",
       "             ('∞', 855),\n",
       "             ('∧', 856),\n",
       "             ('∨', 857),\n",
       "             ('∩', 858),\n",
       "             ('∪', 859),\n",
       "             ('≈', 860),\n",
       "             ('≠', 861),\n",
       "             ('≡', 862),\n",
       "             ('≤', 863),\n",
       "             ('≥', 864),\n",
       "             ('⊂', 865),\n",
       "             ('⊆', 866),\n",
       "             ('⊕', 867),\n",
       "             ('⋅', 868),\n",
       "             ('─', 869),\n",
       "             ('│', 870),\n",
       "             ('■', 871),\n",
       "             ('●', 872),\n",
       "             ('★', 873),\n",
       "             ('☆', 874),\n",
       "             ('☉', 875),\n",
       "             ('♠', 876),\n",
       "             ('♣', 877),\n",
       "             ('♥', 878),\n",
       "             ('♦', 879),\n",
       "             ('♭', 880),\n",
       "             ('♯', 881),\n",
       "             ('⟨', 882),\n",
       "             ('⟩', 883),\n",
       "             ('ⱼ', 884),\n",
       "             ('、', 885),\n",
       "             ('。', 886),\n",
       "             ('《', 887),\n",
       "             ('》', 888),\n",
       "             ('「', 889),\n",
       "             ('」', 890),\n",
       "             ('『', 891),\n",
       "             ('』', 892),\n",
       "             ('〜', 893),\n",
       "             ('い', 894),\n",
       "             ('う', 895),\n",
       "             ('え', 896),\n",
       "             ('お', 897),\n",
       "             ('か', 898),\n",
       "             ('き', 899),\n",
       "             ('く', 900),\n",
       "             ('け', 901),\n",
       "             ('こ', 902),\n",
       "             ('さ', 903),\n",
       "             ('し', 904),\n",
       "             ('す', 905),\n",
       "             ('せ', 906),\n",
       "             ('そ', 907),\n",
       "             ('た', 908),\n",
       "             ('ち', 909),\n",
       "             ('つ', 910),\n",
       "             ('て', 911),\n",
       "             ('と', 912),\n",
       "             ('な', 913),\n",
       "             ('に', 914),\n",
       "             ('の', 915),\n",
       "             ('は', 916),\n",
       "             ('ひ', 917),\n",
       "             ('ま', 918),\n",
       "             ('み', 919),\n",
       "             ('む', 920),\n",
       "             ('め', 921),\n",
       "             ('も', 922),\n",
       "             ('や', 923),\n",
       "             ('ゆ', 924),\n",
       "             ('よ', 925),\n",
       "             ('ら', 926),\n",
       "             ('り', 927),\n",
       "             ('る', 928),\n",
       "             ('れ', 929),\n",
       "             ('ん', 930),\n",
       "             ('ア', 931),\n",
       "             ('ィ', 932),\n",
       "             ('イ', 933),\n",
       "             ('ウ', 934),\n",
       "             ('エ', 935),\n",
       "             ('オ', 936),\n",
       "             ('カ', 937),\n",
       "             ('ガ', 938),\n",
       "             ('キ', 939),\n",
       "             ('ク', 940),\n",
       "             ('グ', 941),\n",
       "             ('コ', 942),\n",
       "             ('サ', 943),\n",
       "             ('シ', 944),\n",
       "             ('ジ', 945),\n",
       "             ('ス', 946),\n",
       "             ('ズ', 947),\n",
       "             ('タ', 948),\n",
       "             ('ダ', 949),\n",
       "             ('ッ', 950),\n",
       "             ('テ', 951),\n",
       "             ('デ', 952),\n",
       "             ('ト', 953),\n",
       "             ('ド', 954),\n",
       "             ('ナ', 955),\n",
       "             ('ニ', 956),\n",
       "             ('ハ', 957),\n",
       "             ('バ', 958),\n",
       "             ('パ', 959),\n",
       "             ('フ', 960),\n",
       "             ('ブ', 961),\n",
       "             ('プ', 962),\n",
       "             ('マ', 963),\n",
       "             ('ミ', 964),\n",
       "             ('ム', 965),\n",
       "             ('ャ', 966),\n",
       "             ('ュ', 967),\n",
       "             ('ラ', 968),\n",
       "             ('リ', 969),\n",
       "             ('ル', 970),\n",
       "             ('レ', 971),\n",
       "             ('ロ', 972),\n",
       "             ('ン', 973),\n",
       "             ('・', 974),\n",
       "             ('ー', 975),\n",
       "             ('一', 976),\n",
       "             ('三', 977),\n",
       "             ('上', 978),\n",
       "             ('下', 979),\n",
       "             ('中', 980),\n",
       "             ('事', 981),\n",
       "             ('二', 982),\n",
       "             ('井', 983),\n",
       "             ('京', 984),\n",
       "             ('人', 985),\n",
       "             ('亻', 986),\n",
       "             ('仁', 987),\n",
       "             ('佐', 988),\n",
       "             ('侍', 989),\n",
       "             ('光', 990),\n",
       "             ('公', 991),\n",
       "             ('力', 992),\n",
       "             ('北', 993),\n",
       "             ('十', 994),\n",
       "             ('南', 995),\n",
       "             ('原', 996),\n",
       "             ('口', 997),\n",
       "             ('史', 998),\n",
       "             ('司', 999),\n",
       "             ...])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f2fa7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try the verbalizer\n",
    "\n",
    "soft_verb = SoftVerbalizer(tokenizer, plm, num_classes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35293840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now load the state dict from saved checkpoint\n",
    "soft_verb.load_state_dict(loaded_model['verbalizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6bb19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6618a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now bring it all together into the prompt classification model\n",
    "\n",
    "trained_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=soft_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47f56ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptForClassification(\n",
       "  (prompt_model): PromptModel(\n",
       "    (plm): BertForMaskedLM(\n",
       "      (bert): BertModel(\n",
       "        (embeddings): BertEmbeddings(\n",
       "          (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "          (position_embeddings): Embedding(512, 768)\n",
       "          (token_type_embeddings): Embedding(2, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): BertEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): BertLayer(\n",
       "              (attention): BertAttention(\n",
       "                (self): BertSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): BertSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): BertIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              )\n",
       "              (output): BertOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cls): BertOnlyMLMHead(\n",
       "        (predictions): BertLMPredictionHead(\n",
       "          (transform): BertPredictionHeadTransform(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (template): ManualTemplate()\n",
       "  )\n",
       "  (verbalizer): SoftVerbalizer(\n",
       "    (head): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=50, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f31ff5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedc6c1",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Implement a test of the trained model through this loading procedure to compare with initial training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2bedbb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix function\n",
    "def plot_confusion_matrix(cm, class_names, save_dir = None):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "       class_names (array, shape = [n]): String names of the integer classes\n",
    "\n",
    "    credit: https://towardsdatascience.com/exploring-confusion-matrix-evolution-on-tensorboard-e66b39f4ac12\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    font = FontProperties()\n",
    "    font.set_family('serif')\n",
    "    font.set_name('Times New Roman')\n",
    "    font.set_style('normal')\n",
    "\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    \n",
    "    # Normalize the confusion matrix.\n",
    "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "    \n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    threshold = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "#     figure.savefig(f'{save_dir}/test_mtx.png')\n",
    "    \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "efba149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plotConfusionMatrix(cm, classes, annot = False):\n",
    "\n",
    "    cf_matrix = cm\n",
    "    df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 10, index=[i for i in classes],\n",
    "                         columns=[i for i in classes])\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    \n",
    "    return sn.heatmap(df_cm, annot=False).get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a21f6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def evaluate(prompt_model, dataloader, class_labels, mode = \"test\", use_cuda = True):\n",
    "    prompt_model.eval()\n",
    "\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in tqdm(enumerate(dataloader), desc = \"evaluating\"):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = prompt_model(inputs)\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    print(f\"accuracy using manual method: {acc}\")\n",
    "\n",
    "    print(alllabels)\n",
    "    print(allpreds)\n",
    "    \n",
    "    print(f\"number unique preds: {len(np.unique(allpreds))}\")\n",
    "    print(f\"number unique labels: {len(np.unique(alllabels))}\")\n",
    "\n",
    "    \n",
    "    # get sklearn based metrics\n",
    "    f1 = f1_score(alllabels, allpreds, average = 'weighted')\n",
    "    prec = precision_score(alllabels, allpreds, average = 'weighted')\n",
    "    recall = recall_score(alllabels, allpreds, average = 'weighted')   \n",
    "    \n",
    "    # get confusion matric\n",
    "#     cm = metrics.confusion_matrix(preds = allpreds,target=alllabels, num_classes =50)\n",
    "    cm = confusion_matrix(alllabels, allpreds)\n",
    "    \n",
    "    print(cm)\n",
    "    \n",
    "    # classification report\n",
    "    print(classification_report(alllabels, allpreds, target_names=class_labels))\n",
    "    \n",
    "#     cm_figure = plotConfusionMatrix(cm, class_labels)\n",
    "    cm_figure = plot_confusion_matrix(cm, class_labels)\n",
    "    \n",
    "    return acc, prec, recall, f1, cm, cm_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b925b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2af6ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_prompt_model(ckpt_dir, plm_type,\n",
    "                              plm_name, template_type,\n",
    "                             template_id, verbalizer_type, verbalizer_id, scripts_path = \"./scripts/\",\n",
    "                             init_from_vocab = True, data_dir = \"../data/intermediary-data/\",\n",
    "                              dataset_name = \"icd9_50\", use_cuda = True):\n",
    "    \n",
    "    '''\n",
    "    Function to reload an already trained promptmodelclassifier. At moment this still requires data/task specific \n",
    "    manual template or verbalizers to be setup as they need to point to correct scripts.\n",
    "    \n",
    "    Args:\n",
    "        ckpt_dir: path to save promptmodel\n",
    "        plm_type: the language model type e.g. bert, t5, gpt\n",
    "        plm_name: the name of the pretrained model/checkpoint e.g. bert-base-uncased\n",
    "        template_type: manual, mixed or soft prompt template?\n",
    "        template_id: which row or idx of the file in scripts will the template be e.g. scripts/manual_template[0] is line 1\n",
    "        verbalizer_type: this maps the tokens (mask prediction) to class labels. Can be one-to-one, many-to-one, \n",
    "                        and can be manual or soft\n",
    "        verbalizer_id: which row or idx of the file in scripts will the verbalizer be e.g. scripts/manual_verbalizer[0] is line 1\n",
    "        scriptnase: path to scripts directory    \n",
    "        init_from_vocab: whether soft templates should be initialized from the plms vocabulary\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # set up datasets first\n",
    "    \n",
    "    dataset_name = dataset_name\n",
    "    data_dir = data_dir\n",
    "    batch_size = 4\n",
    "\n",
    "\n",
    "    dataset = {}\n",
    "    if dataset_name == \"icd9_50\":\n",
    "\n",
    "        logger.warning(f\"Using the following dataset: {dataset_name} \")\n",
    "        Processor = Mimic_ICD9_Processor\n",
    "        # update data_dir\n",
    "        data_dir = f\"{data_dir}/top_50_icd9\"\n",
    "\n",
    "        # get different splits\n",
    "        dataset['train'] = Processor().get_examples(data_dir = data_dir, mode = \"train\")\n",
    "        dataset['validation'] = Processor().get_examples(data_dir = data_dir, mode = \"valid\")\n",
    "        dataset['test'] = Processor().get_examples(data_dir = data_dir, mode = \"test\")[:500]\n",
    "        # the below class labels should align with the label encoder fitted to training data\n",
    "        # you will need to generate this class label text file first using the mimic processor with generate_class_labels flag to set true\n",
    "        # e.g. Processor().get_examples(data_dir = data_dir, mode = \"train\", generate_class_labels = True)[:10000]\n",
    "        class_labels =Processor().load_class_labels()\n",
    "        print(f\"number of classes: {len(class_labels)}\")\n",
    "        scriptsbase = f\"{scripts_path}/mimic_icd9_top50/\"\n",
    "        scriptformat = \"txt\"\n",
    "        max_seq_l = 480 # this should be specified according to the running GPU's capacity \n",
    "\n",
    "        batchsize_t = batch_size\n",
    "        batchsize_e = batch_size\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "\n",
    "    elif dataset_name == \"icd9_triage\":\n",
    "        logger.warning(f\"Using the following dataset: {dataset_name} \")\n",
    "        Processor = Mimic_ICD9_Triage_Processor\n",
    "        # update data_dir\n",
    "        data_dir = f\"{data_dir}/triage\"\n",
    "\n",
    "        # get different splits\n",
    "        dataset['train'] = Processor().get_examples(data_dir = data_dir, mode = \"train\")\n",
    "        dataset['validation'] = Processor().get_examples(data_dir = data_dir, mode = \"valid\")\n",
    "        dataset['test'] = Processor().get_examples(data_dir = data_dir, mode = \"test\")[:500]\n",
    "        # the below class labels should align with the label encoder fitted to training data\n",
    "        # you will need to generate this class label text file first using the mimic processor with generate_class_labels flag to set true\n",
    "        # e.g. Processor().get_examples(data_dir = data_dir, mode = \"train\", generate_class_labels = True)[:10000]\n",
    "        class_labels =Processor().load_class_labels()\n",
    "        print(f\"number of classes: {len(class_labels)}\")\n",
    "        scriptsbase = f\"{scripts_path}/mimic_triage/\"\n",
    "        scriptformat = \"txt\"\n",
    "        max_seq_l = 480 # this should be specified according to the running GPU's capacity \n",
    "\n",
    "        batchsize_t = batch_size\n",
    "        batchsize_e = batch_size\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "    else:\n",
    "        #TODO implement icd9 triage and mimic readmission\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    ######### set up the pretrained model etc ###########\n",
    "    \n",
    "    # initialise the pretrained language model\n",
    "    plm, tokenizer, model_config, WrapperClass = load_plm(plm_type, plm_name)\n",
    "    \n",
    "    # load the already trained prompt model, which will consist of a separate state_dict for the plm/template/verbalizer\n",
    "    loaded_model = torch.load(ckpt_dir)\n",
    "    \n",
    "    \n",
    "    # now load the trained state_dict into the plm model\n",
    "    plm.load_state_dict(loaded_model['plm'])\n",
    "    \n",
    "    \n",
    "    # decide which template and verbalizer to use\n",
    "    if template_type == \"manual\":\n",
    "        print(f\"manual template selected, with id :{template_id}\")\n",
    "        mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(f\"{scriptsbase}/manual_template.txt\", choice=template_id)\n",
    "\n",
    "    elif template_type == \"soft\":\n",
    "        print(f\"soft template selected, with id :{template_id}\")\n",
    "        mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=soft_token_num, initialize_from_vocab=init_from_vocab).from_file(f\"{scriptsbase}/soft_template.txt\", choice=template_id)\n",
    "        # now load the state_dict from ckpt\n",
    "        mytemplate.load_state_dict(loaded_model['template'])\n",
    "\n",
    "    elif template_type == \"mixed\":\n",
    "        print(f\"mixed template selected, with id :{template_id}\")\n",
    "        mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer).from_file(f\"{scriptsbase}/mixed_template.txt\", choice=template_id)\n",
    "        \n",
    "    # now set verbalizer\n",
    "    if verbalizer_type == \"manual\":\n",
    "        print(f\"manual verbalizer selected, with id :{verbalizer_id}\")\n",
    "        myverbalizer = ManualVerbalizer(tokenizer, classes=class_labels).from_file(f\"{scriptsbase}/manual_verbalizer.{scriptformat}\", choice=verbalizer_id)\n",
    "\n",
    "    elif verbalizer_type == \"soft\":\n",
    "        print(f\"soft verbalizer selected!\")\n",
    "        myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=50)\n",
    "        # now load the state dict from saved checkpoint\n",
    "        myverbalizer.load_state_dict(loaded_model['verbalizer'])\n",
    "        \n",
    "    # now bring it all together into the prompt classification model\n",
    "\n",
    "    trained_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer)\n",
    "    \n",
    "    # send to cuda\n",
    "    if use_cuda:\n",
    "        print(\"using cuda!\")\n",
    "        trained_model =  trained_model.cuda()\n",
    "    \n",
    "    # set up mimic data processors\n",
    "    # Below are multiple dataset examples, although right now just mimic ic9-top50. \n",
    "    \n",
    "    # set up test dataloader\n",
    "    test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3, \n",
    "        batch_size=batchsize_e,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "        truncate_method=\"tail\")\n",
    "    \n",
    "    # run evaluation\n",
    "    acc, prec, recall, f1, cm, cm_figure = evaluate(trained_model,test_dataloader, class_labels, \"test\", use_cuda)\n",
    "    \n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n",
    "    \n",
    "    cm_figure.show()\n",
    "    \n",
    "\n",
    "    \n",
    "    return trained_model, dataset, class_labels\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a84af34",
   "metadata": {},
   "source": [
    "at moment this is a bit crude - as hparams have to be manually coded. And the hparams txt files are not easily usable at moment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ae68031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-08 14:56:21.229 | WARNING  | __main__:load_trained_prompt_model:36 - Using the following dataset: icd9_50 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data\n",
      "data path provided was: ../data/intermediary-data//top_50_icd9/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14360it [00:01, 8765.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading valid data\n",
      "data path provided was: ../data/intermediary-data//top_50_icd9/valid.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [00:00, 8973.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test data\n",
      "data path provided was: ../data/intermediary-data//top_50_icd9/test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4754it [00:00, 8876.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual template selected, with id :2\n",
      "soft verbalizer selected!\n",
      "using cuda!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 500it [00:13, 36.52it/s]\n",
      "evaluating: 125it [00:10, 12.23it/s]\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy using manual method: 0.692\n",
      "[30, 39, 13, 48, 18, 26, 33, 21, 2, 24, 16, 38, 43, 13, 37, 6, 31, 48, 13, 4, 29, 13, 1, 25, 30, 22, 32, 23, 13, 13, 4, 22, 12, 44, 12, 13, 16, 13, 13, 48, 47, 13, 22, 32, 13, 27, 34, 22, 15, 30, 17, 48, 1, 13, 4, 5, 25, 10, 13, 4, 13, 4, 7, 16, 4, 21, 34, 19, 47, 17, 22, 40, 36, 1, 14, 31, 30, 13, 16, 4, 33, 13, 3, 47, 11, 13, 4, 13, 13, 47, 42, 13, 37, 12, 34, 13, 21, 6, 16, 12, 27, 4, 13, 24, 7, 46, 27, 47, 13, 13, 27, 10, 48, 22, 32, 4, 28, 32, 44, 2, 16, 16, 19, 13, 13, 12, 25, 11, 5, 10, 4, 46, 20, 47, 14, 21, 16, 12, 15, 13, 11, 44, 16, 47, 30, 4, 47, 32, 13, 9, 29, 16, 7, 2, 31, 12, 22, 46, 32, 14, 15, 38, 24, 2, 32, 12, 7, 18, 12, 37, 13, 40, 12, 48, 25, 24, 4, 12, 19, 35, 4, 4, 47, 47, 47, 15, 32, 23, 4, 47, 13, 10, 12, 41, 4, 19, 12, 40, 11, 36, 22, 20, 5, 47, 32, 13, 17, 13, 47, 18, 37, 4, 14, 21, 22, 35, 16, 4, 43, 33, 47, 13, 39, 35, 18, 45, 46, 13, 11, 11, 12, 19, 7, 32, 46, 13, 46, 30, 19, 29, 31, 28, 47, 31, 16, 48, 22, 13, 22, 47, 22, 4, 37, 13, 13, 27, 24, 44, 12, 12, 13, 46, 46, 16, 47, 13, 22, 37, 32, 4, 12, 20, 48, 46, 31, 38, 36, 47, 4, 10, 13, 20, 13, 4, 13, 30, 48, 13, 3, 47, 4, 15, 48, 31, 48, 22, 15, 2, 24, 24, 47, 10, 47, 12, 13, 10, 43, 27, 31, 33, 16, 40, 46, 42, 12, 3, 5, 13, 47, 16, 4, 15, 28, 24, 4, 4, 13, 48, 11, 26, 42, 16, 17, 25, 1, 4, 43, 45, 13, 4, 32, 36, 12, 16, 16, 17, 4, 13, 12, 2, 47, 46, 4, 10, 14, 4, 13, 31, 11, 4, 47, 12, 31, 22, 13, 4, 13, 13, 45, 12, 13, 16, 19, 39, 22, 47, 13, 13, 13, 13, 0, 12, 21, 49, 19, 27, 13, 48, 30, 39, 13, 15, 12, 47, 12, 16, 44, 7, 14, 36, 16, 33, 47, 12, 12, 47, 10, 21, 10, 22, 47, 20, 20, 47, 9, 47, 12, 4, 18, 4, 29, 5, 24, 16, 13, 13, 9, 18, 20, 16, 27, 30, 46, 12, 4, 13, 21, 4, 42, 13, 7, 27, 23, 22, 19, 12, 13, 15, 46, 16, 14, 47, 13, 12, 25, 12, 32, 13, 47, 46, 49, 13, 13, 32, 46, 4, 6, 13, 4, 20, 47, 44, 46, 21, 30, 13, 43, 16, 39, 40, 34, 22, 13, 26, 22, 40, 21, 40, 48, 13, 19, 22, 19, 46, 16, 48, 1, 40, 32, 13]\n",
      "[19, 39, 13, 48, 18, 25, 32, 22, 2, 24, 16, 38, 4, 13, 37, 6, 33, 48, 16, 4, 29, 15, 4, 8, 30, 22, 32, 42, 13, 13, 41, 22, 10, 44, 12, 12, 16, 13, 13, 48, 47, 15, 22, 4, 13, 27, 40, 21, 15, 32, 17, 48, 41, 12, 31, 5, 25, 10, 13, 4, 13, 30, 7, 16, 32, 21, 34, 19, 47, 17, 22, 35, 37, 4, 14, 31, 30, 13, 13, 1, 6, 13, 4, 47, 11, 13, 4, 15, 11, 46, 42, 13, 37, 13, 40, 13, 21, 6, 16, 12, 27, 1, 13, 13, 7, 46, 27, 47, 13, 13, 27, 12, 48, 25, 32, 4, 28, 32, 44, 4, 16, 16, 20, 13, 13, 30, 25, 10, 5, 10, 4, 46, 32, 48, 4, 22, 16, 12, 13, 13, 11, 44, 16, 47, 30, 5, 47, 6, 13, 16, 29, 16, 6, 37, 32, 13, 22, 31, 4, 14, 15, 3, 24, 4, 4, 12, 22, 18, 12, 37, 13, 29, 12, 48, 26, 24, 4, 12, 15, 40, 2, 32, 46, 47, 47, 15, 25, 23, 4, 47, 13, 10, 13, 41, 30, 16, 13, 40, 11, 36, 22, 19, 5, 47, 32, 13, 17, 13, 47, 12, 37, 4, 14, 21, 22, 35, 16, 4, 43, 33, 47, 13, 39, 35, 18, 45, 46, 13, 11, 11, 15, 32, 7, 12, 46, 24, 46, 4, 13, 29, 32, 28, 47, 31, 16, 48, 22, 13, 22, 47, 22, 31, 37, 13, 13, 27, 24, 44, 12, 12, 13, 46, 46, 16, 47, 13, 25, 37, 32, 6, 12, 20, 48, 46, 12, 38, 40, 47, 32, 10, 13, 19, 13, 32, 13, 30, 48, 13, 4, 47, 38, 15, 48, 32, 48, 4, 15, 39, 24, 24, 47, 10, 47, 31, 13, 13, 4, 27, 32, 32, 16, 35, 46, 42, 13, 32, 5, 13, 47, 16, 4, 15, 28, 24, 31, 4, 13, 47, 11, 32, 42, 16, 18, 25, 43, 4, 43, 45, 12, 14, 12, 36, 12, 16, 16, 17, 31, 13, 40, 4, 47, 46, 4, 10, 14, 31, 13, 31, 11, 4, 47, 12, 30, 22, 13, 2, 13, 28, 45, 12, 12, 16, 19, 39, 21, 47, 13, 13, 13, 13, 45, 28, 21, 49, 17, 27, 13, 48, 30, 39, 16, 15, 12, 47, 13, 16, 44, 7, 32, 36, 16, 33, 47, 12, 12, 47, 10, 21, 12, 26, 47, 19, 20, 47, 16, 47, 12, 32, 18, 4, 29, 30, 24, 16, 13, 13, 9, 18, 43, 16, 27, 4, 46, 32, 4, 13, 21, 4, 42, 13, 7, 27, 42, 22, 4, 12, 13, 15, 46, 16, 14, 47, 13, 13, 25, 12, 32, 13, 47, 46, 49, 13, 13, 32, 46, 40, 6, 13, 4, 18, 47, 45, 46, 21, 32, 13, 40, 16, 39, 35, 44, 22, 13, 25, 22, 37, 21, 36, 47, 13, 12, 22, 19, 46, 16, 48, 20, 34, 32, 12]\n",
      "number unique preds: 49\n",
      "number unique labels: 49\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  1 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 32  1  0]\n",
      " [ 0  0  0 ...  2 13  0]\n",
      " [ 0  0  0 ...  0  0  2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0380       0.00      0.00      0.00         1\n",
      "       03811       0.00      0.00      0.00         5\n",
      "       03842       0.33      0.17      0.22         6\n",
      "       03849       0.00      0.00      0.00         3\n",
      "        0389       0.51      0.45      0.48        40\n",
      "         042       0.80      0.80      0.80         5\n",
      "        1623       0.43      1.00      0.60         3\n",
      "        1983       1.00      0.71      0.83         7\n",
      "       29181       0.00      0.00      0.00         0\n",
      "        3962       1.00      0.33      0.50         3\n",
      "       41011       0.78      0.70      0.74        10\n",
      "       41041       0.88      0.88      0.88         8\n",
      "       41071       0.61      0.58      0.59        33\n",
      "       41401       0.84      0.82      0.83        74\n",
      "       41519       0.83      0.71      0.77         7\n",
      "        4240       0.62      0.89      0.73         9\n",
      "        4241       0.83      0.96      0.89        26\n",
      "        4271       0.80      0.80      0.80         5\n",
      "       42731       0.71      0.83      0.77         6\n",
      "        4280       0.43      0.27      0.33        11\n",
      "       42823       0.50      0.25      0.33         8\n",
      "         430       0.80      0.80      0.80        10\n",
      "         431       0.83      0.71      0.77        21\n",
      "        4321       1.00      0.33      0.50         3\n",
      "       43310       0.89      0.89      0.89         9\n",
      "       43411       0.44      0.67      0.53         6\n",
      "       43491       0.00      0.00      0.00         3\n",
      "        4373       1.00      1.00      1.00         9\n",
      "       44101       0.60      1.00      0.75         3\n",
      "        4414       0.80      1.00      0.89         4\n",
      "         486       0.50      0.50      0.50        10\n",
      "        5070       0.30      0.30      0.30        10\n",
      "       51881       0.30      0.53      0.38        15\n",
      "       51884       0.67      0.40      0.50         5\n",
      "       53240       0.50      0.25      0.33         4\n",
      "       56212       0.40      0.67      0.50         3\n",
      "        5712       0.75      0.60      0.67         5\n",
      "        5715       0.67      1.00      0.80         6\n",
      "        5761       0.67      0.67      0.67         3\n",
      "        5770       0.83      1.00      0.91         5\n",
      "        5789       0.12      0.12      0.12         8\n",
      "        5849       0.33      1.00      0.50         1\n",
      "       85221       0.67      1.00      0.80         4\n",
      "       99662       0.50      0.40      0.44         5\n",
      "       99811       0.83      0.83      0.83         6\n",
      "       99859       0.60      1.00      0.75         3\n",
      "       V3000       0.89      0.94      0.91        17\n",
      "       V3001       0.94      0.91      0.93        35\n",
      "       V3101       0.93      0.87      0.90        15\n",
      "       V3401       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.69       500\n",
      "   macro avg       0.61      0.63      0.60       500\n",
      "weighted avg       0.70      0.69      0.68       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/niallt/venvs/nlp_projects/lib/python3.6/site-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-cde8c9d3f084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m                              \u001b[0mtemplate_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbalizer_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbalizer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscripts_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./scripts/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                              \u001b[0minit_from_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/intermediary-data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                               dataset_name = \"icd9_50\")\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-d27b642757da>\u001b[0m in \u001b[0;36mload_trained_prompt_model\u001b[0;34m(ckpt_dir, plm_type, plm_name, template_type, template_id, verbalizer_type, verbalizer_id, scripts_path, init_from_vocab, data_dir, dataset_name, use_cuda)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_figure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {acc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-c2d43f3fca5c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(prompt_model, dataloader, class_labels, mode, use_cuda)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#     cm_figure = plotConfusionMatrix(cm, class_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mcm_figure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_figure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-101-5efb713439a1>\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(cm, class_names, save_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"white\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"black\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizontalalignment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"center\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# need to refactor the entire training pipeline to store everything including template types etc in a reloadable way\n",
    "# BEST PLAN IS TO SHIFT TO PYTORCH LIGHTNING FOR TRAINING\n",
    "\n",
    "# set up params based on the saved ckpt name\n",
    "ckpt_dir = \"./checkpoints/icd9_50/emilyalsentzer/Bio_ClinicalBERT_tempmanual2_verbsoft0/version_21-01-2022--13-41/checkpoint.ckpt\"\n",
    "plm_type = \"bert\"\n",
    "plm_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "# plm_name = \"bert-base-cased\"\n",
    "template_type = \"manual\"\n",
    "template_id = 2\n",
    "verbalizer_type = \"soft\"\n",
    "verbalizer_id = 0 \n",
    "dataset_name = \"icd9_50\"\n",
    "\n",
    "\n",
    "\n",
    "trained_prompt_model, dataset, class_labels = load_trained_prompt_model(ckpt_dir,plm_type,\n",
    "                              plm_name, template_type,\n",
    "                             template_id, verbalizer_type, verbalizer_id, scripts_path = \"./scripts/\",\n",
    "                             init_from_vocab = True, data_dir = \"../data/intermediary-data/\",\n",
    "                              dataset_name = \"icd9_50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbe8fbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae8c0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-cased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.10.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 28996\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_prompt_model.plm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9b8659f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0380',\n",
       " '03811',\n",
       " '03842',\n",
       " '03849',\n",
       " '0389',\n",
       " '042',\n",
       " '1623',\n",
       " '1983',\n",
       " '29181',\n",
       " '3962',\n",
       " '41011',\n",
       " '41041',\n",
       " '41071',\n",
       " '41401',\n",
       " '41519',\n",
       " '4240',\n",
       " '4241',\n",
       " '4271',\n",
       " '42731',\n",
       " '4280',\n",
       " '42823',\n",
       " '430',\n",
       " '431',\n",
       " '4321',\n",
       " '43310',\n",
       " '43411',\n",
       " '43491',\n",
       " '4373',\n",
       " '44101',\n",
       " '4414',\n",
       " '486',\n",
       " '5070',\n",
       " '51881',\n",
       " '51884',\n",
       " '53240',\n",
       " '56212',\n",
       " '5712',\n",
       " '5715',\n",
       " '5761',\n",
       " '5770',\n",
       " '5789',\n",
       " '5849',\n",
       " '85221',\n",
       " '99662',\n",
       " '99811',\n",
       " '99859',\n",
       " 'V3000',\n",
       " 'V3001',\n",
       " 'V3101',\n",
       " 'V3401']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12469572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f5ccd19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mytemplate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-5512e5af7df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set up test dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtokenizer_wrapper_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWrapperClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_seq_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_max_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchsize_e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_eos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     truncate_method=\"tail\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mytemplate' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run evaluation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_projects",
   "language": "python",
   "name": "nlp_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
