{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39af71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchnlp.encoders import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2d4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48e60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw icd 9 data\n",
    "mimic_data_dir = \"/home/niallt/NLP_DPhil/DPhil_projects/mimic-icd9-classification/clinical-longformer/data/intermediary-data/\"\n",
    "mimic_data = pd.read_csv(f\"{mimic_data_dir}/notes2diagnosis-icd-train.csv_top_codes_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d67f70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>: : : Sex: F Service: CARDIOTHORACIC Allergies...</td>\n",
       "      <td>4240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>: : : Sex: F Service: NEONATOLOGY HISTORY: wee...</td>\n",
       "      <td>V3001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>: : : Sex: M Service: CARDIOTHORACIC Allergies...</td>\n",
       "      <td>41041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>: : : Sex: F Service: MEDICINE Allergies: Peni...</td>\n",
       "      <td>51881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>: : : Sex: F Service: CARDIOTHORACIC Allergies...</td>\n",
       "      <td>3962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14355</th>\n",
       "      <td>28100</td>\n",
       "      <td>: : Service: HISTORY OF THE PRESENT ILLNESS: M...</td>\n",
       "      <td>56212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14356</th>\n",
       "      <td>28101</td>\n",
       "      <td>: : : Sex: F Service: SURGERY Allergies: Patie...</td>\n",
       "      <td>99662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14357</th>\n",
       "      <td>28102</td>\n",
       "      <td>: : Service: CARDIOTHORACIC Allergies: Penicil...</td>\n",
       "      <td>41071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14358</th>\n",
       "      <td>28103</td>\n",
       "      <td>: : : Sex: M Service: Neonatology HISTORY OF P...</td>\n",
       "      <td>V3001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14359</th>\n",
       "      <td>28104</td>\n",
       "      <td>: : : Sex: M Service: CHIEF COMPLAINT: Inguina...</td>\n",
       "      <td>5715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14360 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  label\n",
       "0               1  : : : Sex: F Service: CARDIOTHORACIC Allergies...   4240\n",
       "1               3  : : : Sex: F Service: NEONATOLOGY HISTORY: wee...  V3001\n",
       "2               6  : : : Sex: M Service: CARDIOTHORACIC Allergies...  41041\n",
       "3               7  : : : Sex: F Service: MEDICINE Allergies: Peni...  51881\n",
       "4               8  : : : Sex: F Service: CARDIOTHORACIC Allergies...   3962\n",
       "...           ...                                                ...    ...\n",
       "14355       28100  : : Service: HISTORY OF THE PRESENT ILLNESS: M...  56212\n",
       "14356       28101  : : : Sex: F Service: SURGERY Allergies: Patie...  99662\n",
       "14357       28102  : : Service: CARDIOTHORACIC Allergies: Penicil...  41071\n",
       "14358       28103  : : : Sex: M Service: Neonatology HISTORY OF P...  V3001\n",
       "14359       28104  : : : Sex: M Service: CHIEF COMPLAINT: Inguina...   5715\n",
       "\n",
       "[14360 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26cce0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MimicProcessor(DataProcessor):\n",
    "    # TODO Test needed\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         self.labels = [\"contradiction\", \"entailment\", \"neutral\"]\n",
    "        \n",
    "\n",
    "    def get_examples(self, data_dir):\n",
    "        path = data_dir\n",
    "        examples = []\n",
    "        df = pd.read_csv(path)\n",
    "        self.label_encoder = LabelEncoder(np.unique(df.label).tolist(), reserved_labels = [])\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "#             print(row)\n",
    "            _, body, label = row\n",
    "            label = self.label_encoder.encode(label)\n",
    "#             print(f\"body : {body}\")\n",
    "#             print(f\"label: {label}\")\n",
    "#             print(f\"labels original: {self.label_encoder.index_to_token[label]}\")\n",
    "            \n",
    "            text_a = body.replace('\\\\', ' ')\n",
    "\n",
    "            example = InputExample(\n",
    "                guid=str(idx), text_a=text_a, label=int(label)-1)\n",
    "            examples.append(example)\n",
    "            \n",
    "                \n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78f3892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14360it [00:01, 10144.42it/s]\n"
     ]
    }
   ],
   "source": [
    "data = MimicProcessor().get_examples(data_dir = f\"{mimic_data_dir}/notes2diagnosis-icd-train.csv_top_codes_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfb24f2",
   "metadata": {},
   "source": [
    "# adapt below to work with mimic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained language model (plm)\n",
    "\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7004f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up templates - either manual, knowledgeable or soft\n",
    "from openprompt.prompts import ManualTemplate\n",
    "# mytemplate = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} In this sentence, the topic is {\"mask\"}.')\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(\"scripts/TextClassification/agnews/manual_template.txt\", choice=0)\n",
    "\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) \n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e07a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5295473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from openprompt.prompts import ManualTemplate\n",
    "# mytemplate = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} In this sentence, the topic is {\"mask\"}.')\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(\"scripts/TextClassification/agnews/manual_template.txt\", choice=0)\n",
    "\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) \n",
    "print(wrapped_example)\n",
    "\n",
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, \n",
    "    batch_size=2,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "# next(iter(train_dataloader))\n",
    "\n",
    "# ## Define the verbalizer\n",
    "# In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
    "\n",
    "from openprompt.prompts import SoftVerbalizer, ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "# myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=4,\n",
    "#          label_words=[\"politics\", \"sports\", \"business\", \"technology\"])\n",
    "# or without label words\n",
    "# myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=4)\n",
    "\n",
    "# or manual\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=4).from_file(\"scripts/TextClassification/agnews/manual_verbalizer.txt\")\n",
    "\n",
    "\n",
    "\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# ## below is standard training\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Using different optimizer for prompt parameters and model parameters\n",
    "\n",
    "# optimizer_grouped_parameters2 = [\n",
    "#     {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
    "#     {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
    "# ]\n",
    "\n",
    "\n",
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "# optimizer2 = AdamW(optimizer_grouped_parameters2)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"On epoch: {epoch}\")\n",
    "    tot_loss = 0 \n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        # optimizer2.step()\n",
    "        # optimizer2.zero_grad()\n",
    "        print(tot_loss/(step+1))\n",
    "    \n",
    "# ## evaluate\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"running validation!\")\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, \n",
    "    batch_size=2,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "prompt_model.eval()\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(validation_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"validation:\",acc)\n",
    "\n",
    "\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, \n",
    "    batch_size=2,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(test_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"test:\", acc)  # roughly ~0.85"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
