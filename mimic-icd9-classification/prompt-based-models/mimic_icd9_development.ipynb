{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39af71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import json, csv\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Callable\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.data_utils.data_processor import DataProcessor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchnlp.encoders import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2d4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48e60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 50 icd 9 data\n",
    "\n",
    "# set a local pc directory if not on alejos machines\n",
    "local_pc = False\n",
    "if local_pc:\n",
    "    mimic_data_dir = \"C://Users/ntaylor/Documents/GitHub/Neural_Networks/DPhil_NLP/mimic-icd9-classification/clinical-longformer/data/intermediary-data/top_50_icd9\"\n",
    "else:\n",
    "\n",
    "    mimic_data_dir = \"/home/niallt/NLP_DPhil/DPhil_projects/mimic-icd9-classification/clinical-longformer/data/intermediary-data/top_50_icd9\"\n",
    "mimic_data = pd.read_csv(f\"{mimic_data_dir}/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d67f70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>: : : Sex: F Service: CARDIOTHORACIC Allergies...</td>\n",
       "      <td>4240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>: : : Sex: F Service: NEONATOLOGY HISTORY: wee...</td>\n",
       "      <td>V3001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>: : : Sex: M Service: CARDIOTHORACIC Allergies...</td>\n",
       "      <td>41041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>: : : Sex: F Service: MEDICINE Allergies: Peni...</td>\n",
       "      <td>51881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>: : : Sex: F Service: CARDIOTHORACIC Allergies...</td>\n",
       "      <td>3962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0           1  : : : Sex: F Service: CARDIOTHORACIC Allergies...   4240\n",
       "1           3  : : : Sex: F Service: NEONATOLOGY HISTORY: wee...  V3001\n",
       "2           6  : : : Sex: M Service: CARDIOTHORACIC Allergies...  41041\n",
       "3           7  : : : Sex: F Service: MEDICINE Allergies: Peni...  51881\n",
       "4           8  : : : Sex: F Service: CARDIOTHORACIC Allergies...   3962"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee56b09",
   "metadata": {},
   "source": [
    "fortunatley the label encoder class seems to always map the same code from this data set to same idx. What we want to do now is create a list of the icd9 codes as class names in the order by which the label encoder indexes them. This is useful for the autoprompt pipeline...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95f057fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder(np.unique(mimic_data.label).tolist(), reserved_labels = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10d2391d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0380',\n",
       " '03811',\n",
       " '03842',\n",
       " '03849',\n",
       " '0389',\n",
       " '042',\n",
       " '1623',\n",
       " '1983',\n",
       " '29181',\n",
       " '3962',\n",
       " '41011',\n",
       " '41041',\n",
       " '41071',\n",
       " '41401',\n",
       " '41519',\n",
       " '4240',\n",
       " '4241',\n",
       " '4271',\n",
       " '42731',\n",
       " '4280',\n",
       " '42823',\n",
       " '430',\n",
       " '431',\n",
       " '4321',\n",
       " '43310',\n",
       " '43411',\n",
       " '43491',\n",
       " '4373',\n",
       " '44101',\n",
       " '4414',\n",
       " '486',\n",
       " '5070',\n",
       " '51881',\n",
       " '51884',\n",
       " '53240',\n",
       " '56212',\n",
       " '5712',\n",
       " '5715',\n",
       " '5761',\n",
       " '5770',\n",
       " '5789',\n",
       " '5849',\n",
       " '85221',\n",
       " '99662',\n",
       " '99811',\n",
       " '99859',\n",
       " 'V3000',\n",
       " 'V3001',\n",
       " 'V3101',\n",
       " 'V3401']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icd9_list = list(le.tokens.keys())\n",
    "icd9_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db97433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write these to files as the classes for prompt learning pipeline\n",
    "save_dir = \"../prompt-based-models/scripts/\"\n",
    "\n",
    "textfile = open(f\"{save_dir}/labels.txt\", \"w\")\n",
    "\n",
    "for element in icd9_list:\n",
    "\n",
    "    textfile.write(element + \"\\n\")\n",
    "\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "08458ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0380',\n",
       " '03811',\n",
       " '03842',\n",
       " '03849',\n",
       " '0389',\n",
       " '042',\n",
       " '1623',\n",
       " '1983',\n",
       " '29181',\n",
       " '3962',\n",
       " '41011',\n",
       " '41041',\n",
       " '41071',\n",
       " '41401',\n",
       " '41519',\n",
       " '4240',\n",
       " '4241',\n",
       " '4271',\n",
       " '42731',\n",
       " '4280',\n",
       " '42823',\n",
       " '430',\n",
       " '431',\n",
       " '4321',\n",
       " '43310',\n",
       " '43411',\n",
       " '43491',\n",
       " '4373',\n",
       " '44101',\n",
       " '4414',\n",
       " '486',\n",
       " '5070',\n",
       " '51881',\n",
       " '51884',\n",
       " '53240',\n",
       " '56212',\n",
       " '5712',\n",
       " '5715',\n",
       " '5761',\n",
       " '5770',\n",
       " '5789',\n",
       " '5849',\n",
       " '85221',\n",
       " '99662',\n",
       " '99811',\n",
       " '99859',\n",
       " 'V3000',\n",
       " 'V3001',\n",
       " 'V3101',\n",
       " 'V3401',\n",
       " '']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read back in as list\n",
    "\n",
    "my_file = open(f\"{save_dir}/labels.txt\", \"r\")\n",
    "\n",
    "content = my_file.read().split(\"\\n\")\n",
    "\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26cce0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mimic_ICD9_Processor(DataProcessor):\n",
    "\n",
    "\n",
    "    '''\n",
    "    Function to convert mimic icd9 dataset to a open prompt ready dataset. \n",
    "    \n",
    "    We also instantiate a LabelEncoder() class which is fitted to the given dataset. Fortunately it appears\n",
    "    to create the same mapping for each set, given each set contains all classes. \n",
    "\n",
    "    This is not ideal, and need to think of a better way to store the label encoder based on training data.\n",
    "    \n",
    "\n",
    "  \n",
    "    \n",
    "    '''\n",
    "    # TODO Test needed\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "\n",
    "    def get_examples(self, data_dir, mode = \"train\", label_encoder = None,\n",
    "                     generate_class_labels = False, class_labels_save_dir = \"scripts/\"):\n",
    "\n",
    "        path = f\"{data_dir}/{mode}.csv\"\n",
    "        print(f\"loading {mode} data\")\n",
    "        print(f\"data path provided was: {path}\")\n",
    "        examples = []\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        # need to either initializer and fit the label encoder if not provided\n",
    "        if label_encoder is None:\n",
    "            self.label_encoder = LabelEncoder(np.unique(df.label).tolist(), reserved_labels = [])\n",
    "        else: \n",
    "            print(\"we were given a label encoder\")\n",
    "            self.label_encoder = label_encoder\n",
    "\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows()):\n",
    "#             print(row)\n",
    "            _, body, label = row\n",
    "            label = self.label_encoder.encode(label)\n",
    "#             print(f\"body : {body}\")\n",
    "#             print(f\"label: {label}\")\n",
    "#             print(f\"labels original: {self.label_encoder.index_to_token[label]}\")\n",
    "            \n",
    "            text_a = body.replace('\\\\', ' ')\n",
    "\n",
    "            example = InputExample(\n",
    "                guid=str(idx), text_a=text_a, label=int(label))\n",
    "            examples.append(example)\n",
    "            \n",
    "        logger.info(f\"Returning {len(examples)} samples!\") \n",
    "\n",
    "#         now we want to return a list of the non-encoded labels based on the fitted label encoder\n",
    "        if generate_class_labels:\n",
    "            logger.info(f\"Saving class labels to: {class_labels_save_dir}\")\n",
    "            class_labels = self.generate_class_labels()\n",
    "            # write these to files as the classes for prompt learning pipeline\n",
    "            save_dir = \"../prompt-based-models/scripts/\"\n",
    "\n",
    "            textfile = open(f\"{class_labels_save_dir}/labels_test.txt\", \"w\")\n",
    "\n",
    "            for element in class_labels:\n",
    "\n",
    "                textfile.write(element + \"\\n\")\n",
    "\n",
    "            textfile.close() \n",
    "\n",
    "        return examples\n",
    "\n",
    "    def generate_class_labels(self):\n",
    "        # now we want to return a list of the non-encoded labels based on the fitted label encoder\n",
    "        try:\n",
    "            return list(self.label_encoder.tokens.keys())\n",
    "        except:\n",
    "            print(\"No class labels as haven't fitted any data yet. Run get_examples first!\")\n",
    "            raise NotImplementedError\n",
    "\n",
    "    \n",
    "    def load_class_labels(self, file_path = \"./scripts/labels.txt\"):\n",
    "        # function to load pre-generated class labels\n",
    "        # returns list of class labels\n",
    "\n",
    "        text_file = open(f\"{file_path}\", \"r\")\n",
    "\n",
    "        class_labels = text_file.read().split(\"\\n\")\n",
    "\n",
    "        return class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78f3892a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading valid data\n",
      "data path provided was: /home/niallt/NLP_DPhil/DPhil_projects/mimic-icd9-classification/clinical-longformer/data/intermediary-data/top_50_icd9/valid.csv\n",
      "we were given a label encoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4693it [00:01, 4687.82it/s] \n"
     ]
    }
   ],
   "source": [
    "# get different splits\n",
    "dataset = {}\n",
    "# dataset['train'] = Mimic_ICD9_Processor().get_examples(data_dir = f\"{mimic_data_dir}\", set = \"train\")\n",
    "dataset['valid'] = Mimic_ICD9_Processor().get_examples(data_dir = f\"{mimic_data_dir}\",mode = \"valid\", label_encoder = le, generate_class_labels = True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5c338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2eb169b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['valid'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b214be1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0380',\n",
       " '03811',\n",
       " '03842',\n",
       " '03849',\n",
       " '0389',\n",
       " '042',\n",
       " '1623',\n",
       " '1983',\n",
       " '29181',\n",
       " '3962',\n",
       " '41011',\n",
       " '41041',\n",
       " '41071',\n",
       " '41401',\n",
       " '41519',\n",
       " '4240',\n",
       " '4241',\n",
       " '4271',\n",
       " '42731',\n",
       " '4280',\n",
       " '42823',\n",
       " '430',\n",
       " '431',\n",
       " '4321',\n",
       " '43310',\n",
       " '43411',\n",
       " '43491',\n",
       " '4373',\n",
       " '44101',\n",
       " '4414',\n",
       " '486',\n",
       " '5070',\n",
       " '51881',\n",
       " '51884',\n",
       " '53240',\n",
       " '56212',\n",
       " '5712',\n",
       " '5715',\n",
       " '5761',\n",
       " '5770',\n",
       " '5789',\n",
       " '5849',\n",
       " '85221',\n",
       " '99662',\n",
       " '99811',\n",
       " '99859',\n",
       " 'V3000',\n",
       " 'V3001',\n",
       " 'V3101',\n",
       " 'V3401',\n",
       " '']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = Mimic_ICD9_Processor().load_class_labels()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da70a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26316ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c830fc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dfb24f2",
   "metadata": {},
   "source": [
    "# adapt below to work with mimic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c3a07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained language model (plm)\n",
    "\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "\n",
    "# plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"t5-base\")\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"roberta\", \"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c7004f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'Diagnosis is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': \" : : : Sex: F Service: CARDIOTHORACIC Allergies: Patient recorded as having No Known Allergies to Drugs : Chief Complaint: SOB with exertion, heart murmur since y/o Major Surgical or Invasive Procedure: Mitral valve replacement(mm CE tissue History of Present Illness: y/o female with known MVP who was diagnosed with a heart murmur at age . She was evaluated with serial TTE's which showed worsening MR. Echo showed LVEF % with Mitral valve regurgitant fraction of %. She denies any symptoms. Past Medical History: Hyperlipidemia, MVP/MR, Depression, Obesity Social History: social Etoh, live with mother, deniesDA or tobacco use Family History: noncontributory Physical Exam: y/o F in bed NAD Neuro AA&Ox, nonfocal Chest CTAB resp unlab median sternotomy stable, c/d/i no d/c, RRR no m/r/g chest tubes and epicardial wires removed. Abd S/NT/ND/BS+ EXT warm with trace edema Pertinent Results: RADIOLOGY Preliminary Report CHEST (PA & LAT : AM CHEST (PA & LAT Reason: assess LLL atelectasis MEDICAL CONDITION: year old woman with fever atelectasis seen on prio film REASON FOR THIS EXAMINATION: assess LLL atelectasis INDICATION: Fever, atelectasis seen on prior film. COMPARISONS: . PA and lateral chest radiographs show stable cardiac and mediastinal silhouettes. Again seen are median sternotomy wires and prosthetic mitral valve. There has been interval improvement in the previously seen left retrocardiac opacity suggesting improving atelectasis. No focal opacities are seen. No pleural effusions are seen. IMPRESSION: Improved left retrocardiac opacity suggestive of improving atelectasis. DR. . DR. :AM BLOOD WBC-. RBC-.* Hgb-.* Hct-.* MCV-* MCH-. MCHC-.* RDW-. Plt Ct- :AM BLOOD PT-.* PTT-. INR(PT-. :AM BLOOD Glucose-* UreaN- Creat-. Na- K-. Cl- HCO- AnGap- :AM BLOOD Calcium-. Phos-. Mg-. :AM BLOOD Type-ART pO-* pCO- pH-. calHCO- BaseS- : BLOOD CULTURE AEROBIC BOTTLE (Pending: ANAEROBIC BOTTLE (Pending: Cardiology Report ECHO Study Date of *** Report not finalized *** PRELIMINARY REPORT PATIENT/TEST INFORMATION: Indication: Intra op for MVR Height: (in Weight (lb: BSA (m: . m Status: Inpatient : at : Test: TEE (Complete Doppler: Full Doppler and color Doppler Contrast: None Tape Number: AW-: Test Location: Anesthesia West OR cardiac Technical Quality: Adequate REFERRING DOCTOR: DR. MEASUREMENTS: Left Ventricle - Inferolateral Thickness: . cm (nl . - . cm Left Ventricle - Diastolic Dimension: . cm (nl <= . cm Left Ventricle - Ejection Fraction: % (nl >=% Aorta - Valve Level: . cm (nl <= . cm Aorta - Ascending: . cm (nl <= . cm INTERPRETATION: Findings: LEFT ATRIUM: Marked LA enlargement. RIGHT ATRIUM/INTERATRIAL SEPTUM: A catheter or pacing wire is seen in the RA and extending into the RV. Normal interatrial septum. Prominent Eustachian valve (normal variant. LEFT VENTRICLE: Wall thickness and cavity dimensions were obtained from D images. Normal LV wall thickness. Top normal/borderline dilated LV cavity size. Mild global LV hypokinesis. Mildly depressed LVEF. LV WALL MOTION: Regional LV wall motion abnormalities include: basal anterior - hypo; mid anterior - hypo; basal anteroseptal - hypo; mid anteroseptal - hypo; basal inferoseptal - hypo; mid inferoseptal - hypo; basal inferior - hypo; mid inferior - hypo; basal inferolateral - hypo; mid inferolateral - hypo; basal anterolateral - hypo; mid anterolateral - hypo; anterior apex - hypo; septal apex - hypo; inferior apex - hypo; lateral apex - hypo; apex - hypo; RIGHT VENTRICLE: Normal RV chamber size and free wall motion. AORTA: Normal ascending, transverse and descending thoracic aorta with no atherosclerotic plaque. Normal descending aorta diameter. AORTIC VALVE: ? aortic valve leaflets. Mildly thickened aortic valve leaflets. Mild (+ AR. MITRAL VALVE: Mildly thickened mitral valve leaflets. Myxomatous mitral valve leaflets. Moderate/severe MVP. Mild mitral annular calcification. No MS. Moderate (+ MR. Eccentric MR jet. TRICUSP VALVE: Normal tricuspid valve leaflets. Mild TR. PULMONIC VALVE/PULMONARY ARTERY: Normal pulmonic valve leaflets with physiologic PR. PERICARDIUM: Trivial/physiologic pericardial effusion. GENERAL COMMENTS: A TEE was performed in the location listed above. I certify I was present in compliance with HCFA regulations. No TEE related complications. The patient was under general anesthesia throughout the for the patient. Conclusions: Pre-CPB The left atrium is markedly dilated. Left ventricular wall thicknesses are normal. The left ventricular cavity size is top normal/borderline dilated. There is mild global left ventricular hypokinesis. Overall left ventricular systolic function is mildly depressed EF about %. . Right ventricular chamber size and free wall motion are normal. The ascending, transverse and descending thoracic aorta are small in diameter and free of atherosclerotic plaque. The number of aortic valve leaflets cannot be determined. The aortic valve leaflets are mildly thickened. Trace to mild (+ aortic regurgitation is seen. The mitral valve leaflets are mildly thickened. The mitral valve leaflets are myxomatous. There is moderate/severe posterior mitral valve leaflet prolapse. Mild anterior leaflet prolapse. Moderate (+ mitral regurgitation is seen. The mitral regurgitation jet is eccentric. There is a trivial/physiologic pericardial effusion. Post CPB Normal RV systolic function. LV with continued mild global hypokinesis, EF about %. Mitral bioprosthesis is well seated, normal leaflet function. There is trace valvular and perivalvular MR. . other changes from pre-CPB. Electronically signed by , on :. Cardiology Report ECG Study Date of :: PM Sinus tachycardia. Non-specific ST-T wave changes. Compared to the previous tracing of the rate has increased. Read by: , Intervals Axes Rate PR QRS QT/QTc P QRS T /. - Brief Hospital Course: Ms. was admitted to the on for further management of her dyspnea on exertion. She was taken to the catheterization lab where she was found to have no significant CAD, severe MVP and regurgitation with moderate pulmonary hypertension, LVEF %. Given the severity of her disease, the cardiac surgical service was consulted for surgical repair of her valve disease. She was worked-up in the usual preoperative manner including an echocardiogram which revealed trace Aortic insufficiency, + mitral regurgitation with myxomatous leaflets, and an LV ejection fraction of %, RVEF %, bilateral atrial enlargement. On , Ms. was taken to the operating room. She underwent a mitral valve replacement using a mm pericardial model bioprosthesis. Postoperatively she was taken to the cardiac surgical intensive care unit for monitoring. On postoperative day one, she awoke neurologically intact and was extubated. Beta blockade and aspirin were resumed. She was gently diuresed towards his preoperative weight. On POD Her pressors were weaned, chest tubes were removed, and she was transferred to the cardiac stepdown unit. Beta blockade and aspirin were resumed. She was gently diuresed towards his preoperative weight. On POD her epicardial wires were removed without incident. The physical therapy service was consulted to assist with her postoperative strength and mobility. Her oxygen saturations improved to % on room air. The physical therapy service was consulted to assist with her postoperative strength and mobility. On POD Ms. was kg above her preop weight with good exercise tolerance, no SOB, or Chest pain. Her blood pressure was stable. Her sternotomy incision was clean, dry, and intact without evidence of infection. She was discharged home on POD with services in good condition, cardiac diet, sternal precautions, and instructed to follow up with her PCP and cardiologist in weeks. She will follow up with Dr. in four weeks. Medications on Admission: Paxil mg qday Discharge Medications: . Furosemide mg Tablet Sig: One ( Tablet PO QH (every hours for days. Disp:* Tablet(s* Refills:** . Potassium Chloride mEq Packet Sig: One ( Packet PO QH (every hours for days. Disp:* Packet(s* Refills:** . Docusate Sodium mg Capsule Sig: One ( Capsule PO B ( times a day. Disp:* Capsule(s* Refills:** . Aspirin mg Tablet, Delayed Release (E.C. Sig: One ( Tablet, Delayed Release (E.C. PO DAILY (Daily. Disp:* Tablet, Delayed Release (E.C.(s* Refills:** . Oxycodone-Acetaminophen - mg Tablet Sig: - Tablets PO every - hours as needed for pain. Disp:* Tablet(s* Refills:** . Paroxetine HCl mg Tablet Sig: One ( Tablet PO DAILY (Daily. Disp:* Tablet(s* Refills:** . Amiodarone mg Tablet Sig: Two ( Tablet PO B ( times a day for days: When dose is finished, decrease dose to mg PO daily for days, then decrease dose to mg PO daily. Disp:* Tablet(s* Refills:** . : One ( Tablet PO B ( times a day. Disp:* Tablet(s* Refills:** . Ferrous Gluconate mg Tablet Sig: One ( Tablet PO DAILY (Daily. Disp:* Tablet(s* Refills:** . Ascorbic Acid mg Tablet Sig: One ( Tablet PO B ( times a day. Disp:* Tablet(s* Refills:** . : One ( Tablet PO QH (every hours for days. Disp:* Tablet(s* Refills:** . Ranitidine HCl mg Tablet Sig: One ( Tablet PO B ( times a day. Disp:* Tablet(s* Refills:** . Metoprolol Tartrate mg Tablet Sig: One ( Tablet PO B ( times a day. Disp:* Tablet(s* Refills:** Discharge Disposition: Home Discharge Diagnosis: Mitral regurgitation Discharge Condition: Good. Discharge Instructions: Follow medications on discharge instructions. You may not drive for weeks. You may not lift more than lbs. for months. You should shower daily, let water flow over wounds, pat dry with a towel. Do not use creams, lotions, or powders on wounds. Call our office for sternal drainage, temp.>. Followup Instructions: Make an appointment with Dr. for - weeks . Make an appointment with Dr. for weeks . Make an appointment with your cardiologist - weeks. :\", 'loss_ids': 0, 'shortenable_ids': 1}], {'guid': '0', 'label': 14}]\n"
     ]
    }
   ],
   "source": [
    "# set up templates - either manual, knowledgeable or soft\n",
    "from openprompt.prompts import ManualTemplate\n",
    "# mytemplate = ManualTemplate(tokenizer=tokenizer, text='{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} In this sentence, the topic is {\"mask\"}.')\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer).from_file(\"scripts/manual_template.txt\", choice=0)\n",
    "\n",
    "\n",
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) \n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b30e07a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 500it [00:03, 148.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, \n",
    "    batch_size=2,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"tail\")\n",
    "# next(iter(train_dataloader))\n",
    "\n",
    "# ## Define the verbalizer\n",
    "# In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:\n",
    "\n",
    "from openprompt.prompts import SoftVerbalizer, ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "# myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=4,\n",
    "#          label_words=[\"politics\", \"sports\", \"business\", \"technology\"])\n",
    "# or without label words\n",
    "myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=50)\n",
    "\n",
    "# or manual\n",
    "# myverbalizer = ManualVerbalizer(tokenizer, num_classes=4).from_file(\"scripts/TextClassification/agnews/manual_verbalizer.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9699230b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftVerbalizer(\n",
       "  (head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=50, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myverbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5295473",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# ## below is standard training\n",
    "\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters1 = [\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# Using different optimizer for prompt parameters and model parameters\n",
    "\n",
    "# optimizer_grouped_parameters2 = [\n",
    "#     {'params': prompt_model.verbalizer.group_parameters_1, \"lr\":3e-5},\n",
    "#     {'params': prompt_model.verbalizer.group_parameters_2, \"lr\":3e-4},\n",
    "# ]\n",
    "\n",
    "\n",
    "optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)\n",
    "# optimizer2 = AdamW(optimizer_grouped_parameters2)\n",
    "\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"On epoch: {epoch}\")\n",
    "    tot_loss = 0 \n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer1.step()\n",
    "        optimizer1.zero_grad()\n",
    "        # optimizer2.step()\n",
    "        # optimizer2.zero_grad()\n",
    "        print(tot_loss/(step+1))\n",
    "    \n",
    "# ## evaluate\n",
    "\n",
    "# %%\n",
    "\n",
    "print(\"running validation!\")\n",
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, \n",
    "    batch_size=2,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "\n",
    "prompt_model.eval()\n",
    "\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(validation_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"validation:\",acc)\n",
    "\n",
    "\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer, \n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, \n",
    "    batch_size=2,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")\n",
    "allpreds = []\n",
    "alllabels = []\n",
    "with torch.no_grad():\n",
    "    for step, inputs in enumerate(test_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(\"test:\", acc)  # roughly ~0.85"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
