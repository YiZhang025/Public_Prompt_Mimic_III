{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded admissions file, no doing preprocessing of that file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntaylor\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3049: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\ntaylor\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 43880/43880 [31:43<00:00, 23.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 44112/44112 [48:36<00:00, 15.13it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 44551/44551 [1:31:59<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producing train/val/test splits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = \"Z:/Shared Data/MIMIC-III/\"\n",
    "df_adm = pd.read_csv(f'{data_dir}ADMISSIONS.csv/ADMISSIONS.csv')\n",
    "\n",
    "print(\"loaded admissions file, now doing preprocessing of that file\")\n",
    "df_adm.ADMITTIME = pd.to_datetime(df_adm.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "df_adm.DISCHTIME = pd.to_datetime(df_adm.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "df_adm.DEATHTIME = pd.to_datetime(df_adm.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "\n",
    "df_adm = df_adm.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "df_adm = df_adm.reset_index(drop = True)\n",
    "df_adm['NEXT_ADMITTIME'] = df_adm.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "df_adm['NEXT_ADMISSION_TYPE'] = df_adm.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)\n",
    "\n",
    "rows = df_adm.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
    "df_adm.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
    "df_adm.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN\n",
    "\n",
    "df_adm = df_adm.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "\n",
    "#When we filter out the \"ELECTIVE\", we need to correct the next admit time for these admissions since there might be 'emergency' next admit after \"ELECTIVE\"\n",
    "df_adm[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = df_adm.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')\n",
    "df_adm['DAYS_NEXT_ADMIT']=  (df_adm.NEXT_ADMITTIME - df_adm.DISCHTIME).dt.total_seconds()/(24*60*60)\n",
    "df_adm['OUTPUT_LABEL'] = (df_adm.DAYS_NEXT_ADMIT < 30).astype('int')\n",
    "### filter out newborn and death\n",
    "df_adm = df_adm[df_adm['ADMISSION_TYPE']!='NEWBORN']\n",
    "df_adm = df_adm[df_adm.DEATHTIME.isnull()]\n",
    "df_adm['DURATION'] = (df_adm['DISCHTIME']-df_adm['ADMITTIME']).dt.total_seconds()/(24*60*60)\n",
    "\n",
    "df_notes = pd.read_csv(f\"{data_dir}NOTEEVENTS.csv/NOTEEVENTS.csv\")\n",
    "df_notes = df_notes.sort_values(by=['SUBJECT_ID','HADM_ID','CHARTDATE'])\n",
    "df_adm_notes = pd.merge(df_adm[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME','OUTPUT_LABEL','DURATION']],\n",
    "                        df_notes[['SUBJECT_ID','HADM_ID','CHARTDATE','TEXT','CATEGORY']], \n",
    "                        on = ['SUBJECT_ID','HADM_ID'],\n",
    "                        how = 'left')\n",
    "\n",
    "df_adm_notes.ADMITTIME_C = df_adm_notes.ADMITTIME.apply(lambda x: str(x).split(' ')[0])\n",
    "df_adm_notes['ADMITTIME_C'] = pd.to_datetime(df_adm_notes.ADMITTIME_C, format = '%Y-%m-%d', errors = 'coerce')\n",
    "df_adm_notes['CHARTDATE'] = pd.to_datetime(df_adm_notes.CHARTDATE, format = '%Y-%m-%d', errors = 'coerce')\n",
    "\n",
    "\n",
    "### If Discharge Summary \n",
    "df_discharge = df_adm_notes[df_adm_notes['CATEGORY'] == 'Discharge summary']\n",
    "# multiple discharge summary for one admission -> after examination -> replicated summary -> replace with the last one \n",
    "df_discharge = (df_discharge.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
    "df_discharge=df_discharge[df_discharge['TEXT'].notnull()]\n",
    "\n",
    "### If Less than n days on admission notes (Early notes)\n",
    "def less_n_days_data (df_adm_notes, n):\n",
    "    \n",
    "    df_less_n = df_adm_notes[((df_adm_notes['CHARTDATE']-df_adm_notes['ADMITTIME_C']).dt.total_seconds()/(24*60*60))<n]\n",
    "    df_less_n=df_less_n[df_less_n['TEXT'].notnull()]\n",
    "    #concatenate first\n",
    "    df_concat = pd.DataFrame(df_less_n.groupby('HADM_ID')['TEXT'].apply(lambda x: \"%s\" % ' '.join(x))).reset_index()\n",
    "    df_concat['OUTPUT_LABEL'] = df_concat['HADM_ID'].apply(lambda x: df_less_n[df_less_n['HADM_ID']==x].OUTPUT_LABEL.values[0])\n",
    "\n",
    "    return df_concat\n",
    "\n",
    "df_less_2 = less_n_days_data(df_adm_notes, 2)\n",
    "df_less_3 = less_n_days_data(df_adm_notes, 3)\n",
    "\n",
    "import re\n",
    "def preprocess1(x):\n",
    "    y=re.sub('\\\\[(.*?)\\\\]','',x) #remove de-identified brackets\n",
    "    y=re.sub('[0-9]+\\.','',y) #remove 1.2. since the segmenter segments based on this\n",
    "    y=re.sub('dr\\.','doctor',y)\n",
    "    y=re.sub('m\\.d\\.','md',y)\n",
    "    y=re.sub('admission date:','',y)\n",
    "    y=re.sub('discharge date:','',y)\n",
    "    y=re.sub('--|__|==','',y)\n",
    "    return y\n",
    "\n",
    "def preprocessing(df_less_n): \n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].fillna(' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\n',' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.replace('\\r',' ')\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].apply(str.strip)\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].str.lower()\n",
    "\n",
    "    df_less_n['TEXT']=df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
    "\n",
    "    #to get 318 words chunks for readmission tasks\n",
    "    from tqdm import tqdm\n",
    "    df_len = len(df_less_n)\n",
    "    want=pd.DataFrame({'ID':[],'TEXT':[],'Label':[]})\n",
    "    for i in tqdm(range(df_len)):\n",
    "        x=df_less_n.TEXT.iloc[i].split()\n",
    "        n=int(len(x)/318)\n",
    "        for j in range(n):\n",
    "            want=want.append({'TEXT':' '.join(x[j*318:(j+1)*318]),'Label':df_less_n.OUTPUT_LABEL.iloc[i],'ID':df_less_n.HADM_ID.iloc[i]},ignore_index=True)\n",
    "        if len(x)%318>10:\n",
    "            want=want.append({'TEXT':' '.join(x[-(len(x)%318):]),'Label':df_less_n.OUTPUT_LABEL.iloc[i],'ID':df_less_n.HADM_ID.iloc[i]},ignore_index=True)\n",
    "    \n",
    "    return want\n",
    "\n",
    "df_discharge = preprocessing(df_discharge)\n",
    "df_less_2 = preprocessing(df_less_2)\n",
    "df_less_3 = preprocessing(df_less_3)\n",
    "\n",
    "### An example to get the train/test/split with random state:\n",
    "### note that we divide on patient admission level and share among experiments, instead of notes level. \n",
    "### This way, since our methods run on the same set of admissions, we can see the\n",
    "### progression of readmission scores. \n",
    "\n",
    "readmit_ID = df_adm[df_adm.OUTPUT_LABEL == 1].HADM_ID\n",
    "not_readmit_ID = df_adm[df_adm.OUTPUT_LABEL == 0].HADM_ID\n",
    "#subsampling to get the balanced pos/neg numbers of patients for each dataset\n",
    "not_readmit_ID_use = not_readmit_ID.sample(n=len(readmit_ID), random_state=1)\n",
    "id_val_test_t=readmit_ID.sample(frac=0.2,random_state=1)\n",
    "id_val_test_f=not_readmit_ID_use.sample(frac=0.2,random_state=1)\n",
    "\n",
    "id_train_t = readmit_ID.drop(id_val_test_t.index)\n",
    "id_train_f = not_readmit_ID_use.drop(id_val_test_f.index)\n",
    "\n",
    "id_val_t=id_val_test_t.sample(frac=0.5,random_state=1)\n",
    "id_test_t=id_val_test_t.drop(id_val_t.index)\n",
    "\n",
    "id_val_f=id_val_test_f.sample(frac=0.5,random_state=1)\n",
    "id_test_f=id_val_test_f.drop(id_val_f.index)\n",
    "\n",
    "# test if there is overlap between train and test, should return \"array([], dtype=int64)\"\n",
    "(pd.Index(id_test_t).intersection(pd.Index(id_train_t))).values\n",
    "\n",
    "id_test = pd.concat([id_test_t, id_test_f])\n",
    "test_id_label = pd.DataFrame(data = list(zip(id_test, [1]*len(id_test_t)+[0]*len(id_test_f))), columns = ['id','label'])\n",
    "\n",
    "id_val = pd.concat([id_val_t, id_val_f])\n",
    "val_id_label = pd.DataFrame(data = list(zip(id_val, [1]*len(id_val_t)+[0]*len(id_val_f))), columns = ['id','label'])\n",
    "\n",
    "id_train = pd.concat([id_train_t, id_train_f])\n",
    "train_id_label = pd.DataFrame(data = list(zip(id_train, [1]*len(id_train_t)+[0]*len(id_train_f))), columns = ['id','label'])\n",
    "\n",
    "#get discharge train/val/test\n",
    "print(\"producing train/val/test splits\")\n",
    "\n",
    "discharge_train = df_discharge[df_discharge.ID.isin(train_id_label.id)]\n",
    "discharge_val = df_discharge[df_discharge.ID.isin(val_id_label.id)]\n",
    "discharge_test = df_discharge[df_discharge.ID.isin(test_id_label.id)]\n",
    "\n",
    "# subsampling for training....since we obtain training on patient admission level so now we have same number of pos/neg readmission\n",
    "# but each admission is associated with different length of notes and we train on each chunks of notes, not on the admission, we need\n",
    "# to balance the pos/neg chunks on training set. (val and test set are fine) Usually, positive admissions have longer notes, so we need \n",
    "# find some negative chunks of notes from not_readmit_ID that we haven't used yet\n",
    "\n",
    "df = pd.concat([not_readmit_ID_use, not_readmit_ID])\n",
    "df = df.drop_duplicates(keep=False)\n",
    "#check to see if there are overlaps\n",
    "(pd.Index(df).intersection(pd.Index(not_readmit_ID_use))).values\n",
    "\n",
    "# for this set of split with random_state=1, we find we need 400 more negative training samples\n",
    "not_readmit_ID_more = df.sample(n=400, random_state=1)\n",
    "discharge_train_snippets = pd.concat([df_discharge[df_discharge.ID.isin(not_readmit_ID_more)], discharge_train])\n",
    "\n",
    "#shuffle\n",
    "discharge_train_snippets = discharge_train_snippets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "#check if balanced\n",
    "discharge_train_snippets.Label.value_counts()\n",
    "\n",
    "# MOVED TO SEPARATE CELL\n",
    "# print(f\"writing discharge_train_snippets to data/discharge/train.csv\")\n",
    "# discharge_train_snippets.to_csv('./data/discharge/train.csv')\n",
    "# discharge_val.to_csv('./data/discharge/val.csv')\n",
    "# discharge_test.to_csv('.data//discharge/test.csv')\n",
    "\n",
    "### for Early notes experiment: we only need to find training set for 3 days, then we can test \n",
    "### both 3 days and 2 days. Since we split the data on patient level and experiments share admissions\n",
    "### in order to see the progression, the 2 days training dataset is a subset of 3 days training set.\n",
    "### So we only train 3 days and we can test/val on both 2 & 3days or any time smaller than 3 days. This means\n",
    "### if we train on a dataset with all the notes in n days, we can predict readmissions smaller than n days. \n",
    "\n",
    "#for 3 days note, similar to discharge\n",
    "\n",
    "early_train = df_less_3[df_less_3.ID.isin(train_id_label.id)]\n",
    "not_readmit_ID_more = df.sample(n=500, random_state=1)\n",
    "early_train_snippets = pd.concat([df_less_3[df_less_3.ID.isin(not_readmit_ID_more)], early_train])\n",
    "#shuffle\n",
    "early_train_snippets = early_train_snippets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "# early_train_snippets.to_csv('./data/3days/train.csv')\n",
    "\n",
    "early_val = df_less_3[df_less_3.ID.isin(val_id_label.id)]\n",
    "# early_val.to_csv('./3days/val.csv')\n",
    "\n",
    "# we want to test on admissions that are not discharged already. So for less than 3 days of notes experiment,\n",
    "# we filter out admissions discharged within 3 days\n",
    "actionable_ID_3days = df_adm[df_adm['DURATION'] >= 3].HADM_ID\n",
    "test_actionable_id_label = test_id_label[test_id_label.id.isin(actionable_ID_3days)]\n",
    "early_test = df_less_3[df_less_3.ID.isin(test_actionable_id_label.id)]\n",
    "\n",
    "# early_test.to_csv('./data/3days/test.csv')\n",
    "\n",
    "#for 2 days notes, we only obtain test set. Since the model parameters are tuned on the val set of 3 days\n",
    "\n",
    "actionable_ID_2days = df_adm[df_adm['DURATION'] >= 2].HADM_ID\n",
    "\n",
    "test_actionable_id_label_2days = test_id_label[test_id_label.id.isin(actionable_ID_2days)]\n",
    "\n",
    "early_test_2days = df_less_2[df_less_2.ID.isin(test_actionable_id_label_2days.id)]\n",
    "\n",
    "# early_test_2days.to_csv('./data/2days/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing discharge train snippets to ./data/discharge/train.csv\n",
      "writing discharge_val to ./data/discharge/val.csv\n",
      "writing discharge_test to ./data/discharge/test.csv\n",
      "writing early_train_snippets to ./data/3days/train.csv\n",
      "writing early_val to ./data/3days/val.csv\n",
      "writing early_test to ./data/3days/test.csv\n",
      "writing early_test_2days to ./data/2days/test.csv\n"
     ]
    }
   ],
   "source": [
    "# separated the writing of files to csv to avoid directory/path error\n",
    "\n",
    "# discharge summary data pre processed\n",
    "print(f\"writing discharge train snippets to ./data/discharge/train.csv\")\n",
    "discharge_train_snippets.to_csv('./data/discharge/train.csv')\n",
    "print(f\"writing discharge_val to ./data/discharge/val.csv\")\n",
    "discharge_val.to_csv('./data/discharge/val.csv')\n",
    "print(f\"writing discharge_test to ./data/discharge/test.csv\")\n",
    "discharge_test.to_csv('./data/discharge/test.csv')\n",
    "\n",
    "# 3 day note dataset - for early notes experiment\n",
    "print(f\"writing early_train_snippets to ./data/3days/train.csv\")\n",
    "early_train_snippets.to_csv('./data/3days/train.csv')\n",
    "print(f\"writing early_val to ./data/3days/val.csv\")\n",
    "early_val.to_csv('./data/3days/val.csv')\n",
    "\n",
    "# 3 day test\n",
    "print(f\"writing early_test to ./data/3days/test.csv\")\n",
    "early_test.to_csv('./data/3days/test.csv')\n",
    "\n",
    "\n",
    "# 2 day test\n",
    "print(f\"writing early_test_2days to ./data/2days/test.csv\")\n",
    "early_test_2days.to_csv('./data/2days/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discharge.to_pickle(\"./data/pickle/df_discharge.pkl\")\n",
    "df_less_2.to_pickle(\"./data/pickle/df_less_2.pkl\")\n",
    "df_less_3.to_pickle(\"./data/pickle/df_less_3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ./run_readmission.py \\\n",
    "#   --task_name readmission \\\n",
    "#   --do_train \\\n",
    "#   --do_eval \\\n",
    "#   --data_dir ./data/discharge \\\n",
    "#   --bert_model ./model/pretraining \\\n",
    "#   --max_seq_length 512 \\\n",
    "#   --train_batch_size 32 \\\n",
    "#   --learning_rate 2e-5 \\\n",
    "#   --num_train_epochs 10 \\\n",
    "#   --output_dir ./result_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
